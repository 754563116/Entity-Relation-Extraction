{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrJodGBrdQIZ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2973,
     "status": "ok",
     "timestamp": 1563764635339,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "4dP5RrUNHC67",
    "outputId": "6f755eb3-e1fb-4be5-bf05-9632016e82cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  #注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1544,
     "status": "ok",
     "timestamp": 1563764641537,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "-PTkbsy0Hdlt",
    "outputId": "91123d4f-2273-41ca-9d0f-ed7156eadc76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1563781766497,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "6-007Cx-1RhB",
    "outputId": "a54f1cb2-3715-4c32-9ebd-7a11de1723dc"
   },
   "outputs": [],
   "source": [
    "\n",
    "#coding:utf8\n",
    "import codecs\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque#双端队列  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unknown': 0, '父母': 1, '夫妻': 2, '师生': 3, '兄弟姐妹': 4, '合作': 5, '情侣': 6, '祖孙': 7, '好友': 8, '亲戚': 9, '同门': 10, '上下级': 11}\n"
     ]
    }
   ],
   "source": [
    "Relation2id = {'unknown': 0, '父母': 1, '夫妻': 2, \n",
    "               '师生': 3, '兄弟姐妹': 4, '合作': 5, \n",
    "               '情侣': 6, '祖孙': 7, '好友': 8,\n",
    "               '亲戚': 9, '同门': 10, '上下级': 11}\n",
    "print(Relation2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4i8755K1VHV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20400 20400 20400 20400\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "         \n",
    "#处理数据集\n",
    "datas = deque()#记录单词\n",
    "labels = deque()\n",
    "positionE1 = deque()\n",
    "positionE2 = deque()\n",
    "pos_tag = deque()#记录所有词性\n",
    "parsers = deque()#记录依存分析特征\n",
    "Roles = []#记录语义角色\n",
    "e1_role = []\n",
    "e2_role = []\n",
    "total_data=0\n",
    "count = [0,0,0,0,0,0,0,0,0,0,0,0]#\n",
    "\n",
    "#加载分词相关文件\n",
    "LTP_DATA_DIR = 'E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\ltp_data_v3.4.0'  # ltp模型目录的路径\n",
    "from pyltp import Segmentor\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "#加载词性相关文件\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "#加载句法分析相关文件\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "from pyltp import Parser\n",
    "parser = Parser() # 初始化实例\n",
    "parser.load(par_model_path)  # 加载模型\n",
    "\n",
    "#加载语义角色标注相关文件\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl_win.model')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。\n",
    "from pyltp import SementicRoleLabeller\n",
    "labeller = SementicRoleLabeller() # 初始化实例\n",
    "labeller.load(srl_model_path)  # 加载模型\n",
    "wt = open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\parsers.txt','w',encoding='utf-8')\n",
    "wr = open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\\\roles.txt','w',encoding='utf-8')\n",
    "with codecs.open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\data.txt','r','utf-8') as tfc: \n",
    "    for lines in tfc:\n",
    "        line = lines.split()#切分\n",
    "        \n",
    "        #print(line[2])\n",
    "        if count[Relation2id[line[2]]] <1700:\n",
    "\n",
    "            sentence = []\n",
    "            #index:获取实体在句子中的索引\n",
    "            index1 = line[3].index(line[0])\n",
    "            #print(index1)\n",
    "            position1 = []\n",
    "            index2 = line[3].index(line[1])\n",
    "            #print(index2)\n",
    "            position2 = []\n",
    "           \n",
    "            #pyltp分词\n",
    "            #print(line[3])\n",
    "         \n",
    "            words = segmentor.segment(line[3])  # 分词\n",
    "          #  print(line[3])\n",
    "            #print('|'.join(words))\n",
    "           # words = ' '.join(words)\n",
    "           # print(words)\n",
    "            words= \" \".join(words).split()\n",
    "            \n",
    "            \"\"\"\n",
    "            #去停用词\n",
    "            for word in words:\n",
    "              if word not in stpwrdlst:\n",
    "                 #print(word)\n",
    "                 clean_words.append(word)\"\"\"\n",
    "            \n",
    "            #pylt词性标注 \n",
    "            postags = postagger.postag(words)  # 词性标注\n",
    "            #print(postags)\n",
    "            postags= ' '.join(postags).split()\n",
    "            #print(postags)\n",
    "            pos_tag.append(postags)\n",
    "            \n",
    "            #依存句法分析\n",
    "            arcs = parser.parse(words, postags)  # 句法分析\n",
    "            \n",
    "            for arc in arcs:\n",
    "               #arc.relation = ' '.join(arc.relation)\n",
    "               #print(arc.relation)\n",
    "               parsers.append(arc.relation)\n",
    "               wt.write(arc.relation)\n",
    "               wt.write(' ')\n",
    "            wt.write('\\n')\n",
    "            \"\"\"\n",
    "            for  i in range(len(parsers)):\n",
    "                 if parsers[i] == 0:\n",
    "                  e1_pos.append(i - index1)\n",
    "                  e2_pos.append(i - index2)\n",
    "            #print(e1_pos)     \n",
    "      \n",
    "            \"\"\"\n",
    "            #print( parsers)\n",
    "         \n",
    "            #语义角色标注\n",
    "            roles = labeller.label(words, postags, arcs)  # 语义角色标注\n",
    "            for role in roles:\n",
    "                for arg in role.arguments:\n",
    "                   #print(arg.name)\n",
    "                   wr.write(arg.name)\n",
    "                   wr.write(' ')\n",
    "            wr.write('\\n') \n",
    "                   # Roles.append(arg.name)\n",
    "             #print(Roles)       \n",
    "           # e1_role.append(Roles[index1])\n",
    "           # e2_role.append(Roles[index2])\n",
    "            #print(Roles)\n",
    "          \n",
    "\n",
    "            for i,word in enumerate(words):\n",
    "                sentence.append(word)\n",
    "                #print(word,sentence)\n",
    "                #print(sentence)\n",
    "                position1.append(i-3-index1)\n",
    "                position2.append(i-3-index2)\n",
    "                i+=1\n",
    "            datas.append(sentence)\n",
    "           # print(line[2])\n",
    "            labels.append(Relation2id[line[2]])\n",
    "            #print(labels)\n",
    "            positionE1.append(position1)\n",
    "            positionE2.append(position2)\n",
    "        count[Relation2id[line[2]]]+=1   \n",
    "        total_data+=1\n",
    "    \n",
    "segmentor.release()  # 释放模型 \n",
    "postagger.release() \n",
    "#recognizer.release() \n",
    "parser.release()  \n",
    "labeller.release()  \n",
    "print (len(datas),len(pos_tag),len(positionE1),len(positionE2))\n",
    "#返回的是队列对象，需要通过pop()出栈操作去访问队列值或者通过循环访问队列值\n",
    "\n",
    "#print(datas.pop())\n",
    "#for tag in pos_tag:\n",
    "#    print(tag)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5058,
     "status": "ok",
     "timestamp": 1563779261745,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "On0Vy6_V6fr4",
    "outputId": "c5264391-3f32-411c-9b60-760e03499e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20400\n"
     ]
    }
   ],
   "source": [
    "relation = []\n",
    "with  codecs.open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\parsers.txt','r',encoding='utf-8') as f:\n",
    "  for lines in f:\n",
    "     # print(lines)\n",
    "      line =lines.split()\n",
    "      relation.append(line) \n",
    "print(len(relation)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1563769680614,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "mVeMotQsCwKj",
    "outputId": "8e4348a3-52de-4be9-ce6e-dfdd4e681f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20400\n"
     ]
    }
   ],
   "source": [
    "roles = []\n",
    "with  codecs.open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\\\roles.txt','r',encoding='utf-8') as f:\n",
    "  for lines in f:\n",
    "    \n",
    "      line =lines.split()\n",
    "      #print(lines)\n",
    "      roles.append(line) \n",
    "print(len(roles))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1182,
     "status": "ok",
     "timestamp": 1555065341594,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "Fvx9HcVDF_0Z",
    "outputId": "6b26a10f-325f-4601-92af-ed152c8fcd1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1         A1\n",
      "2         A0\n",
      "3        ADV\n",
      "4        TMP\n",
      "5        DIS\n",
      "6        LOC\n",
      "7         A2\n",
      "8        MNR\n",
      "9        DIR\n",
      "10       PRP\n",
      "11       BNF\n",
      "12       EXT\n",
      "13       TPC\n",
      "14      C-A0\n",
      "15        A3\n",
      "16      C-A1\n",
      "17       FRQ\n",
      "18       CND\n",
      "19        A4\n",
      "20     BLANK\n",
      "21    UNKNOW\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#处理语义角色特征\n",
    "#通过pd.Series将parers里的词做成id-relation\n",
    "role = flatten(roles)\n",
    "sr_allrole = pd.Series(role)\n",
    "\n",
    "#统计每个依存关系出现的次数\n",
    "sr_allrole = sr_allrole.value_counts()\n",
    "#print(sr_allpostag)\n",
    "#去重\n",
    "set_role = sr_allrole.index\n",
    "#print(set_words)\n",
    "#生成id集合\n",
    "set_roleids = range(1, len(set_role)+1)\n",
    "#print(set_ids[:5])\n",
    "#生成postag-id表,去重\n",
    "role2id = pd.Series(set_roleids, index=set_role)\n",
    "#print(word2id)\n",
    "#生成id-relation表\n",
    "id2role = pd.Series(set_role, index=set_roleids)\n",
    "\n",
    "#在e1_relation2id表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "role2id[\"BLANK\"]=len(role2id)+1\n",
    "role2id[\"UNKNOW\"]=len(role2id)+1\n",
    "#在e1_id2relation表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "id2role[len(id2role)+1]=\"BLANK\"\n",
    "id2role[len(id2role)+1]=\"UNKNOW\"\n",
    "print(id2role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMy6CthO86_k"
   },
   "source": [
    "id表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2788,
     "status": "ok",
     "timestamp": 1555065017144,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "_zs0s-LVA5-E",
    "outputId": "ba34d0a6-6808-4960-f4e0-b85d13a51c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        ATT\n",
      "2         WP\n",
      "3        COO\n",
      "4        ADV\n",
      "5        VOB\n",
      "6        SBV\n",
      "7        RAD\n",
      "8        HED\n",
      "9        POB\n",
      "10       LAD\n",
      "11       CMP\n",
      "12       DBL\n",
      "13       FOB\n",
      "14       IOB\n",
      "15     BLANK\n",
      "16    UNKNOW\n",
      "dtype: object\n",
      "id2word 1             ，\n",
      "2             、\n",
      "3             的\n",
      "4             。\n",
      "5             ：\n",
      "6             是\n",
      "7             在\n",
      "8             -\n",
      "9             与\n",
      "10            《\n",
      "11            》\n",
      "12            （\n",
      "13            了\n",
      "14            和\n",
      "15            ）\n",
      "16            为\n",
      "17            “\n",
      "18            ”\n",
      "19            一\n",
      "20            人\n",
      "21            之\n",
      "22            有\n",
      "23            等\n",
      "24            后\n",
      "25            他\n",
      "26            中\n",
      "27            不\n",
      "28            也\n",
      "29            于\n",
      "30            聽\n",
      "          ...  \n",
      "39739       上弦月\n",
      "39740        汉市\n",
      "39741       冯异想\n",
      "39742       黑龙江\n",
      "39743        后记\n",
      "39744       史学科\n",
      "39745        缺口\n",
      "39746      甄子丹汪\n",
      "39747       白宝山\n",
      "39748       织毛袜\n",
      "39749      代杉重矩\n",
      "39750       西城区\n",
      "39751        蓉帐\n",
      "39752       欧莱雅\n",
      "39753        温情\n",
      "39754        照面\n",
      "39755       温兆伦\n",
      "39756       傅海峰\n",
      "39757        隔帘\n",
      "39758        瑜利\n",
      "39759        胡闹\n",
      "39760        方冰\n",
      "39761      立锥之地\n",
      "39762        临哭\n",
      "39763        半州\n",
      "39764        探析\n",
      "39765       擒孟达\n",
      "39766        康述\n",
      "39767     BLANK\n",
      "39768    UNKNOW\n",
      "Length: 39768, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#将队列里的列表展平\n",
    "import collections\n",
    "def flatten(x):\n",
    "    def iselement(e):\n",
    "        return not(isinstance(e, collections.Iterable) and not isinstance(e, str))\n",
    "    for el in x:\n",
    "        if iselement(el):\n",
    "            yield el\n",
    "        else:\n",
    "            yield from flatten(el) \n",
    "\n",
    "\"\"\"\n",
    "def flatten(x):    \n",
    "    result = [] \n",
    "    for el in x:        \n",
    "        if isinstance(x, collections.Iterable) and not isinstance(el, str):            \n",
    "            result.extend(flatten(el))        \n",
    "        else:            \n",
    "            result.append(el)    \n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "print (all_words.__next__)\n",
    "for i in all_words:\n",
    "    print(i)\n",
    "\"\"\"\n",
    "#处理依存分析特征\n",
    "#通过pd.Series将parers里的词做成id-relation\n",
    "sr_allrelation = pd.Series(parsers)\n",
    "\n",
    "#统计每个依存关系出现的次数\n",
    "sr_allrelation = sr_allrelation.value_counts()\n",
    "#print(sr_allpostag)\n",
    "#去重\n",
    "set_relation = sr_allrelation.index\n",
    "#print(set_words)\n",
    "#生成id集合\n",
    "set_relationids = range(1, len(set_relation)+1)\n",
    "#print(set_ids[:5])\n",
    "#生成postag-id表,去重\n",
    "relation2id = pd.Series(set_relationids, index=set_relation)\n",
    "#print(word2id)\n",
    "#生成id-relation表\n",
    "id2relation = pd.Series(set_relation, index=set_relationids)\n",
    "\n",
    "#在e1_relation2id表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "relation2id[\"BLANK\"]=len(relation2id)+1\n",
    "relation2id[\"UNKNOW\"]=len(relation2id)+1\n",
    "#在e1_id2relation表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "id2relation[len(id2relation)+1]=\"BLANK\"\n",
    "id2relation[len(id2relation)+1]=\"UNKNOW\"\n",
    "print(id2relation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#使用列表推导式将列表平铺成1行\n",
    "all_pos_tag = [num for elem in pos_tag for num in elem]\n",
    "#处理词性特征\n",
    "#通过pd.Series将all_pos_tag里的词做成id-词性\n",
    "sr_allpostag = pd.Series(all_pos_tag)\n",
    "#print(sr_allpostag)\n",
    "#统计每个词性出现的次数\n",
    "sr_allpostag = sr_allpostag.value_counts()\n",
    "#print(sr_allpostag)\n",
    "#把词性单独提出来，形成词性集合\n",
    "set_postag = sr_allpostag.index\n",
    "#print(set_words)\n",
    "#生成id集合\n",
    "set_postagids = range(1, len(set_postag)+1)\n",
    "#print(set_ids[:5])\n",
    "#生成postag-id表,去重\n",
    "postag2id = pd.Series(set_postagids, index=set_postag)\n",
    "#print(word2id)\n",
    "#生成id-postag表\n",
    "id2postag = pd.Series(set_postag, index=set_postagids)\n",
    "#print(id2word)\n",
    "#在postag2id表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "postag2id[\"BLANK\"]=len(postag2id)+1\n",
    "postag2id[\"UNKNOW\"]=len(postag2id)+1\n",
    "#在id2postag表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "id2postag[len(id2postag)+1]=\"BLANK\"\n",
    "id2postag[len(id2postag)+1]=\"UNKNOW\"\n",
    "#print (\"id2postag\",id2postag)\n",
    "#print (len(id2postag))\n",
    "#print (postag2id)\n",
    "\n",
    "\n",
    "#返回的是生成器对象,需要通过__next__()获取或者循环读出\n",
    "all_words = flatten(datas)\n",
    "#处理words\n",
    "#通过pd.Series将all_words里的词做成id-词\n",
    "sr_allwords = pd.Series(all_words)\n",
    "#print(sr_allwords)\n",
    "#统计每个词出现的次数\n",
    "sr_allwords = sr_allwords.value_counts()\n",
    "#print(sr_allwords)\n",
    "#把词单独提出来，形成词集合，去重\n",
    "set_words = sr_allwords.index\n",
    "#print(set_words)\n",
    "#生成id集合\n",
    "set_ids = range(1, len(set_words)+1)\n",
    "#print(set_ids[:5])\n",
    "#生成word-id表\n",
    "word2id = pd.Series(set_ids, index=set_words)\n",
    "#print(word2id)\n",
    "#生成id-word表\n",
    "id2word = pd.Series(set_words, index=set_ids)\n",
    "#print(id2word)\n",
    "#在word2id表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "word2id[\"BLANK\"]=len(word2id)+1\n",
    "word2id[\"UNKNOW\"]=len(word2id)+1\n",
    "#在id2word表的末尾加上\"BLANK\"和\"UNKNOW\"\n",
    "id2word[len(id2word)+1]=\"BLANK\"\n",
    "id2word[len(id2word)+1]=\"UNKNOW\"\n",
    "print (\"id2word\",id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q51isbQlkhXj"
   },
   "outputs": [],
   "source": [
    "max_len = 50#定义序列长度\n",
    "#处理word，把word转成id\n",
    "#不够序列长度的，进行padding\n",
    "def X_padding(words):\n",
    "    \"\"\"把 words 转为 id 形式，并自动补全位 max_len 长度。\"\"\"\n",
    "    ids = []\n",
    "    #如果词在word2id表中，则返回其id,否则返回未知词\"UNKNOW\"的id\n",
    "    for i in words:\n",
    "        if i in word2id:\n",
    "            ids.append(word2id[i])\n",
    "        else:\n",
    "            ids.append(word2id[\"UNKNOW\"])\n",
    "    #如果ids的长度大于等于最大序列长度则将超出的部分截取掉，否则将ids进行扩展 \n",
    "    if len(ids) >= max_len: \n",
    "        return ids[:max_len]\n",
    "    ids.extend([word2id[\"BLANK\"]]*(max_len-len(ids))) \n",
    "\n",
    "    return ids\n",
    "def p_padding(pos):\n",
    "    \"\"\"把 词性 转为 id 形式，并自动补全位 max_len 长度。\"\"\"\n",
    "    ids = []\n",
    "    #如果词在word2id表中，则返回其id,否则返回未知词\"UNKNOW\"的id\n",
    "    for i in pos:\n",
    "        if i in postag2id:\n",
    "            ids.append(postag2id[i])\n",
    "        else:\n",
    "            ids.append(postag2id[\"UNKNOW\"])\n",
    "    #如果ids的长度大于等于最大序列长度则将超出的部分截取掉，否则将ids进行扩展 \n",
    "    if len(ids) >= max_len: \n",
    "        return ids[:max_len]\n",
    "    ids.extend([postag2id[\"BLANK\"]]*(max_len-len(ids))) \n",
    "\n",
    "    return ids\n",
    "\n",
    "def R_padding(relation):\n",
    "    \"\"\"把 依存特征 转为 id 形式，并自动补全位 max_len 长度。\"\"\"\n",
    "    ids = []\n",
    "    #如果词在word2id表中，则返回其id,否则返回未知词\"UNKNOW\"的id\n",
    "    for i in relation:\n",
    "        if i in relation2id:\n",
    "            ids.append(relation2id[i])\n",
    "        else:\n",
    "            ids.append(relation2id[\"UNKNOW\"])\n",
    "    #如果ids的长度大于等于最大序列长度则将超出的部分截取掉，否则将ids进行扩展 \n",
    "    if len(ids) >= max_len: \n",
    "        return ids[:max_len]\n",
    "    ids.extend([relation2id[\"BLANK\"]]*(max_len-len(ids))) \n",
    "\n",
    "    return ids\n",
    "def Role_padding(role):\n",
    "    \"\"\"把 依存特征 转为 id 形式，并自动补全位 max_len 长度。\"\"\"\n",
    "    ids = []\n",
    "    #如果词在word2id表中，则返回其id,否则返回未知词\"UNKNOW\"的id\n",
    "    for i in role:\n",
    "        if i in role2id:\n",
    "            ids.append(role2id[i])\n",
    "        else:\n",
    "            ids.append(role2id[\"UNKNOW\"])\n",
    "    #如果ids的长度大于等于最大序列长度则将超出的部分截取掉，否则将ids进行扩展 \n",
    "    if len(ids) >= max_len: \n",
    "        return ids[:max_len]\n",
    "    ids.extend([role2id[\"BLANK\"]]*(max_len-len(ids))) \n",
    "\n",
    "    return ids\n",
    " \n",
    "#处理位置特征\n",
    "def pos(num):\n",
    "    if num<-40:\n",
    "        return 0\n",
    "    if num>=-40 and num<=40:\n",
    "        return num+40\n",
    "    if num>40:\n",
    "        return 80\n",
    "     \n",
    "#位置特征padding\n",
    "def position_padding(words):\n",
    "    words = [pos(i) for i in words]\n",
    "    if len(words) >= max_len:  \n",
    "        return words[:max_len]\n",
    "    words.extend([81]*(max_len-len(words))) \n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4318
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36372,
     "status": "ok",
     "timestamp": 1555065968617,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "PmpTZZZg1a86",
    "outputId": "3ad9e35c-7693-4a27-e758-25cf5ec8e982"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fa1fb2771524>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#用pandas处理数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tags'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'positionE1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpositionE1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'positionE2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpositionE2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'pos'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'relation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrelation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'role'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mroles\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#word和pos这两列的值应用自己定义的X_padding函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_padding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#用pandas处理数据\n",
    "df_data = pd.DataFrame({'words': datas, 'tags': labels,'positionE1':positionE1,'positionE2':positionE2,'pos':pos_tag,'relation':relation,'role':roles}, index=range(len(datas)))\n",
    "#word和pos这两列的值应用自己定义的X_padding函数\n",
    "df_data['words'] = df_data['words'].apply(X_padding)\n",
    "df_data['pos'] = df_data['pos'].apply(p_padding)\n",
    "df_data['relation'] = df_data['relation'].apply(R_padding)\n",
    "df_data['role'] = df_data['role'].apply(Role_padding)\n",
    "print(df_data['words'])\n",
    "print(df_data['pos'])\n",
    "print(df_data['relation'])\n",
    "print(df_data['role'])\n",
    "df_data['tags'] = df_data['tags']\n",
    "df_data['positionE1'] = df_data['positionE1'].apply(position_padding)\n",
    "df_data['positionE2'] = df_data['positionE2'].apply(position_padding)\n",
    "\n",
    "datas = np.asarray(list(df_data['words'].values))#单词\n",
    "labels = np.asarray(list(df_data['tags'].values))#类别标签\n",
    "positionE1 = np.asarray(list(df_data['positionE1'].values))#位置1\n",
    "positionE2 = np.asarray(list(df_data['positionE2'].values))#位置2\n",
    "pos = np.asarray(list(df_data['pos'].values))#单词词性\n",
    "relation = np.asarray(list(df_data['relation'].values))\n",
    "role = np.asarray(list(df_data['role'].values))\n",
    "print(relation.shape)\n",
    "print(datas.shape)\n",
    "print(labels.shape)\n",
    "print(positionE1.shape)\n",
    "print(positionE2.shape)\n",
    "print(pos.shape)\n",
    "print(role.shape)\n",
    "\n",
    "#开始保存\n",
    "import pickle\n",
    "with open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\people_relation_train.pkl', 'wb') as outp:\n",
    "\tpickle.dump(word2id, outp)\n",
    "\tpickle.dump(id2word, outp)\n",
    "\tpickle.dump(postag2id, outp)\n",
    "\tpickle.dump(Relation2id, outp) \n",
    "\tpickle.dump(datas, outp)\n",
    "\tpickle.dump(labels, outp)\n",
    "\tpickle.dump(positionE1, outp)\n",
    "\tpickle.dump(positionE2, outp)\n",
    "\tpickle.dump(pos, outp)\n",
    "\tpickle.dump(relation, outp)\n",
    "\tpickle.dump(role, outp)  \n",
    "print ('** Finished saving the train data.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cb6u0oMFefKT"
   },
   "source": [
    "\n",
    "\n",
    "处理测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 680366,
     "status": "ok",
     "timestamp": 1555067830018,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "YHwAU4KYa_WG",
    "outputId": "b83f9b51-a5c3-47e6-bfe7-27597b22a657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#处理和保存测试数据\n",
    "\n",
    "datas = deque()\n",
    "labels = deque()\n",
    "positionE1 = deque()\n",
    "positionE2 = deque()\n",
    "pos_tag =deque()#记录所有词性\n",
    "\n",
    "e1_role_t = []\n",
    "e2_role_t = []\n",
    "count = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\"\"\"\n",
    "clean_words = []#接收除去停用词的单词\n",
    "#从文件导入停用词表\n",
    "stpwrdpath = \"drive/stopwords_sum.txt\"\n",
    "stpwrd_dic = open(stpwrdpath, 'rb')\n",
    "stpwrd_content = stpwrd_dic.read()\n",
    "#将停用词表转换为list  \n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "print(len(stpwrdlst))\n",
    "stpwrd_dic.close()\n",
    "\"\"\"\n",
    "#加载分词相关文件\n",
    "LTP_DATA_DIR = 'E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\ltp_data_v3.4.0'  # ltp模型目录的路径\n",
    "from pyltp import Segmentor\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "#加载词性相关文件\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "#加载句法分析相关文件\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "from pyltp import Parser\n",
    "parser = Parser() # 初始化实例\n",
    "parser.load(par_model_path)  # 加载模型\n",
    "\n",
    "#加载语义角色标注相关文件\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl_win.model')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。\n",
    "from pyltp import SementicRoleLabeller\n",
    "labeller = SementicRoleLabeller() # 初始化实例\n",
    "labeller.load(srl_model_path)  # 加载模型\n",
    "wt = open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\parsers_t.txt','w',encoding='utf-8')\n",
    "wr = open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\\\roles_t.txt','w',encoding='utf-8')\n",
    "with codecs.open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\data.txt','r','utf-8') as tfc: \n",
    "    for lines in tfc:\n",
    "        line = lines.split()#切分\n",
    "        #关系类别数大于1500并且小于等于1800的，作为测试样本\n",
    "        if count[Relation2id[line[2]]] >1500 and count[Relation2id[line[2]]]<=1800:\n",
    "        #if count[relation2id[line[2]]] <=1500:\n",
    "            sentence = []\n",
    "            index1 = line[3].index(line[0])\n",
    "            position1 = []\n",
    "            index2 = line[3].index(line[1])\n",
    "            position2 = []\n",
    "            #pyltp分词\n",
    "            words = segmentor.segment(line[3])  # 分词\n",
    "            words= \" \".join(words).split()\n",
    "            \"\"\"\n",
    "            #去停用词\n",
    "            for word in words:\n",
    "              if word not in stpwrdlst:\n",
    "                 #print(word)\n",
    "                 clean_words.append(word)\"\"\"\n",
    "            \n",
    "            #pylt词性标注 \n",
    "            postags = postagger.postag(words)  # 词性标注\n",
    "            postags= ' '.join(postags).split()\n",
    "            #print(postags)\n",
    "            pos_tag.append(postags)\n",
    "            \n",
    "            #依存句法分析\n",
    "            arcs = parser.parse(words, postags)  # 句法分析\n",
    "            for arc in arcs:\n",
    "               #arc.relation = ' '.join(arc.relation)\n",
    "               wt.write(arc.relation)\n",
    "               wt.write(' ')\n",
    "           \n",
    "            wt.write('\\n' ) \n",
    "            \"\"\"\n",
    "            for  i in range(len(parsers)):\n",
    "                 if parsers[i] == 0:\n",
    "                  e1_pos.append(i - index1)\n",
    "                  e2_pos.append(i - index2)\n",
    "            #print(e1_pos)     \n",
    "      \n",
    "            \"\"\"\n",
    "            #print( parsers)\n",
    "            \n",
    "            #语义角色标注\n",
    "            roles = labeller.label(words, postags, arcs)  # 语义角色标注\n",
    "            for role in roles:\n",
    "              for arg in role.arguments:\n",
    "                  #Roles.append(arg.name)\n",
    "                  wr.write(arg.name)\n",
    "                  wr.write(' ')\n",
    "           \n",
    "            wr.write('\\n' ) \n",
    "           # print(Roles)       \n",
    "          #  e1_role.append(Roles[index1])\n",
    "           # e2_role.append(Roles[index2])\n",
    "           # print(Roles)\n",
    "         \n",
    "            for i,word in enumerate(words):\n",
    "                sentence.append(word)\n",
    "                position1.append(i-3-index1)\n",
    "                position2.append(i-3-index2)\n",
    "                i+=1\n",
    "            datas.append(sentence)\n",
    "            labels.append(Relation2id[line[2]])\n",
    "            positionE1.append(position1)\n",
    "            positionE2.append(position2)\n",
    "        count[Relation2id[line[2]]]+=1\n",
    "segmentor.release()  # 释放模型 \n",
    "postagger.release()\n",
    "parser.release()\n",
    "print(len(datas))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3927,
     "status": "ok",
     "timestamp": 1555067899288,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "aJfj-iJIc5Nj",
    "outputId": "d5aafdd4-0d2b-4e9f-93e7-07226b52e7b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    }
   ],
   "source": [
    "relation = []\n",
    "with  codecs.open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\parsers_t.txt','r',encoding='utf-8') as f:\n",
    "  for lines in f:\n",
    "      line =lines.split()\n",
    "      relation.append(line) \n",
    "print(len(relation)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1134,
     "status": "ok",
     "timestamp": 1555067905508,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "2i2aO_m1KsAW",
    "outputId": "75ff921d-f781-4d6b-e777-5d22bbfe1883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    }
   ],
   "source": [
    "roles = []\n",
    "with  codecs.open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\\\roles_t.txt','r',encoding='utf-8') as f:\n",
    "  for lines in f:\n",
    "      line =lines.split()\n",
    "      roles.append(line) \n",
    "print(len(roles)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3158
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9392,
     "status": "ok",
     "timestamp": 1555068241511,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "IT-00_Poaw_0",
    "outputId": "9f65c19e-59ed-4c69-ad68-62bde2e8deb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [15, 11683, 703, 12054, 12515, 1632, 12, 1406,...\n",
      "1       [1144, 140, 9, 255, 13522, 2530, 1, 919, 381, ...\n",
      "2       [26627, 22, 4957, 2, 5249, 2, 30588, 2, 32392,...\n",
      "3       [9, 1007, 22, 96, 53, 14791, 5209, 9, 2500, 3,...\n",
      "4       [81, 13, 69, 4236, 20, 1329, 446, 2048, 744, 4...\n",
      "5       [2, 1889, 2, 956, 10, 39258, 624, 3947, 11, 20...\n",
      "6       [87, 1157, 3, 33664, 3910, 764, 1, 31582, 339,...\n",
      "7       [7, 17, 7174, 18, 26, 1, 2392, 354, 655, 3, 11...\n",
      "8       [6646, 6272, 5638, 216, 15411, 1, 61, 659, 147...\n",
      "9       [7597, 9902, 566, 21990, 5, 43, 7, 521, 26, 83...\n",
      "10      [537, 3137, 1904, 3, 17, 2844, 4893, 18, 1, 43...\n",
      "11      [34133, 3324, 5, 33180, 2, 25923, 2, 24643, 2,...\n",
      "12      [44, 95, 42, 43, 6065, 8, 127, 124, 9, 5790, 2...\n",
      "13      [688, 2, 1456, 4134, 2745, 1, 16, 4805, 326, 1...\n",
      "14      [200, 60, 120, 825, 342, 43, 1233, 350, 6432, ...\n",
      "15      [869, 29, 321, 7, 19, 82, 224, 162, 26, 1, 504...\n",
      "16      [177, 73, 5, 6271, 3, 73, 22, 6789, 2, 8364, 2...\n",
      "17      [204, 41, 314, 1065, 30, 1299, 146, 5, 275, 28...\n",
      "18      [4121, 2084, 403, 2640, 206, 2148, 1, 1194, 60...\n",
      "19      [2, 26520, 10, 33823, 70, 3, 7387, 11, 315, 10...\n",
      "20      [8, 22240, 48, 136, 522, 87, 3601, 6494, 45, 1...\n",
      "21      [1742, 11, 12, 4719, 1, 29, 5017, 1, 5689, 15,...\n",
      "22      [1, 37, 15609, 13555, 1, 210, 37, 4065, 3777, ...\n",
      "23      [211, 5566, 336, 234, 9649, 3, 34981, 2, 25208...\n",
      "24      [9993, 391, 6887, 1286, 57, 27, 6, 1059, 14, 1...\n",
      "25      [5567, 104, 117, 1121, 92, 112, 1, 62, 273, 39...\n",
      "26      [21053, 13964, 187, 29, 1611, 1, 7, 337, 365, ...\n",
      "27      [2176, 1203, 6674, 49, 1, 812, 10950, 19, 5258...\n",
      "28      [10476, 21, 180, 12, 6778, 141, 9170, 2, 8777,...\n",
      "29      [2, 278, 1698, 230, 1, 177, 62, 22, 5, 230, 2,...\n",
      "                              ...                        \n",
      "3570    [8, 146, 682, 85, 29776, 12809, 21, 109, 85, 1...\n",
      "3571    [1371, 721, 6, 1310, 3, 15885, 1, 61, 759, 139...\n",
      "3572    [37, 20247, 29228, 1308, 31358, 9640, 1, 87, 1...\n",
      "3573    [23769, 6, 1833, 3, 248, 3102, 3, 65, 1, 118, ...\n",
      "3574    [1918, 101, 1473, 2760, 91, 80, 3, 6, 4943, 4,...\n",
      "3575    [2072, 9, 903, 134, 16, 27, 913, 81, 3, 19, 13...\n",
      "3576    [10839, 5, 8798, 216, 1, 13143, 1, 561, 156, 6...\n",
      "3577    [7, 5076, 3, 22295, 70, 1, 2406, 487, 35608, 2...\n",
      "3578    [871, 2, 2869, 2, 2917, 48, 20, 1253, 6, 2843,...\n",
      "3579    [38731, 8, 1271, 138, 33089, 2, 55, 4039, 302,...\n",
      "3580    [11197, 1516, 722, 1, 710, 9, 53, 1093, 338, 7...\n",
      "3581    [977, 193, 15, 30, 958, 5, 43, 1387, 12, 98, 5...\n",
      "3582    [10524, 108, 7, 11179, 1965, 169, 11817, 1, 16...\n",
      "3583    [8852, 10807, 1135, 5, 6626, 30, 188, 5, 5153,...\n",
      "3584    [1481, 435, 14, 1328, 40, 2603, 1481, 544, 1, ...\n",
      "3585    [1439, 6, 7283, 3, 6666, 4, 39767, 39767, 3976...\n",
      "3586    [2239, 1040, 7320, 8405, 5807, 1, 5105, 543, 9...\n",
      "3587    [8, 103, 313, 6980, 14, 11449, 6980, 3, 2118, ...\n",
      "3588    [8, 2191, 3, 53, 3742, 3, 1596, 1884, 5156, 1,...\n",
      "3589    [44, 204, 42, 11096, 8, 177, 610, 695, 5715, 9...\n",
      "3590    [8, 9, 554, 708, 193, 24, 1, 554, 2, 706, 239,...\n",
      "3591    [3679, 8, 64, 2696, 5713, 2424, 1854, 6, 4807,...\n",
      "3592    [871, 2, 2869, 2, 2843, 48, 20, 1253, 6, 2917,...\n",
      "3593    [2970, 6, 604, 3039, 628, 1, 1579, 986, 874, 8...\n",
      "3594    [350, 1009, 3, 55, 5846, 1, 47, 6, 1755, 3, 56...\n",
      "3595    [8, 1217, 1383, 7, 10, 2390, 11, 26, 3, 1075, ...\n",
      "3596    [3572, 1, 1752, 3591, 13, 1500, 8478, 17284, 3...\n",
      "3597    [926, 3058, 240, 10161, 1, 2358, 2, 11392, 7, ...\n",
      "3598    [2405, 553, 953, 2976, 36, 9361, 15, 319, 1, 9...\n",
      "3599    [1274, 1, 12392, 1220, 18461, 1, 53, 25517, 1,...\n",
      "Name: words, Length: 3600, dtype: object\n",
      "0       [1, 4, 4, 4, 4, 4, 1, 2, 14, 3, 1, 4, 4, 4, 3,...\n",
      "1       [11, 6, 7, 4, 4, 3, 1, 11, 3, 5, 2, 1, 4, 2, 3...\n",
      "2       [2, 3, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, ...\n",
      "3       [7, 2, 3, 8, 2, 4, 4, 7, 10, 5, 2, 4, 3, 8, 17...\n",
      "4       [3, 5, 17, 8, 2, 2, 1, 2, 2, 1, 1, 6, 7, 4, 4,...\n",
      "5       [1, 4, 1, 4, 1, 3, 3, 2, 1, 2, 1, 4, 1, 3, 5, ...\n",
      "6       [7, 4, 5, 3, 21, 3, 1, 4, 7, 6, 3, 14, 18, 1, ...\n",
      "7       [7, 1, 19, 1, 14, 1, 4, 3, 4, 5, 3, 6, 3, 3, 5...\n",
      "8       [4, 6, 3, 2, 4, 1, 11, 6, 6, 14, 1, 2, 4, 3, 3...\n",
      "9       [10, 10, 14, 10, 1, 6, 7, 13, 14, 2, 2, 3, 2, ...\n",
      "10      [7, 3, 3, 5, 1, 21, 2, 1, 1, 6, 3, 5, 4, 1, 4,...\n",
      "11      [2, 2, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, ...\n",
      "12      [1, 8, 1, 6, 4, 1, 2, 3, 7, 2, 3, 2, 4, 4, 7, ...\n",
      "13      [4, 1, 4, 17, 2, 1, 7, 2, 2, 3, 5, 2, 1, 4, 6,...\n",
      "14      [2, 2, 4, 20, 4, 6, 4, 4, 4, 2, 3, 2, 2, 2, 4,...\n",
      "15      [6, 7, 2, 7, 8, 15, 2, 3, 14, 1, 3, 5, 4, 1, 2...\n",
      "16      [17, 2, 1, 4, 5, 2, 3, 4, 1, 4, 1, 4, 1, 4, 1,...\n",
      "17      [8, 15, 10, 10, 3, 2, 2, 1, 10, 10, 1, 9, 2, 8...\n",
      "18      [2, 12, 2, 3, 2, 3, 1, 11, 2, 2, 4, 1, 4, 1, 4...\n",
      "19      [1, 4, 1, 2, 14, 5, 2, 1, 4, 4, 1, 4, 1, 4, 1,...\n",
      "20      [1, 2, 8, 15, 2, 7, 2, 3, 3, 3, 2, 2, 3, 16, 5...\n",
      "21      [2, 1, 1, 4, 1, 7, 2, 1, 4, 1, 1, 9, 2, 1, 1, ...\n",
      "22      [1, 7, 4, 2, 1, 6, 7, 4, 3, 1, 11, 21, 5, 2, 6...\n",
      "23      [13, 2, 3, 3, 4, 5, 2, 1, 4, 5, 2, 1, 4, 6, 3,...\n",
      "24      [4, 3, 4, 4, 6, 6, 3, 4, 11, 4, 5, 9, 2, 1, 11...\n",
      "25      [2, 7, 13, 18, 3, 2, 1, 2, 3, 4, 1, 4, 1, 4, 1...\n",
      "26      [10, 4, 3, 7, 13, 1, 7, 13, 14, 4, 6, 7, 4, 5,...\n",
      "27      [4, 3, 4, 2, 1, 3, 21, 6, 3, 1, 8, 15, 3, 1, 1...\n",
      "28      [2, 5, 2, 1, 4, 4, 9, 1, 4, 1, 4, 1, 4, 1, 1, ...\n",
      "29      [1, 17, 2, 4, 1, 17, 2, 3, 1, 4, 1, 4, 1, 4, 1...\n",
      "                              ...                        \n",
      "3570    [1, 2, 4, 1, 4, 4, 5, 2, 1, 4, 5, 2, 1, 28, 28...\n",
      "3571    [4, 11, 3, 4, 5, 2, 1, 11, 2, 6, 7, 4, 9, 8, 1...\n",
      "3572    [7, 2, 2, 3, 3, 2, 1, 7, 2, 3, 6, 3, 5, 2, 1, ...\n",
      "3573    [4, 3, 2, 5, 2, 3, 5, 2, 1, 7, 3, 2, 3, 2, 6, ...\n",
      "3574    [4, 7, 4, 3, 6, 9, 5, 3, 2, 1, 28, 28, 28, 28,...\n",
      "3575    [4, 7, 4, 6, 3, 6, 3, 3, 5, 8, 15, 2, 8, 2, 2,...\n",
      "3576    [4, 1, 19, 2, 1, 4, 1, 10, 13, 13, 2, 3, 2, 4,...\n",
      "3577    [7, 4, 5, 3, 14, 1, 4, 3, 2, 6, 16, 1, 11, 6, ...\n",
      "3578    [4, 1, 4, 1, 4, 8, 2, 6, 3, 4, 5, 2, 1, 4, 3, ...\n",
      "3579    [4, 1, 2, 2, 4, 1, 2, 4, 6, 3, 2, 2, 1, 28, 28...\n",
      "3580    [3, 2, 2, 1, 4, 7, 2, 4, 2, 3, 1, 10, 10, 10, ...\n",
      "3581    [2, 3, 1, 2, 2, 1, 6, 4, 1, 13, 2, 1, 3, 8, 2,...\n",
      "3582    [4, 2, 7, 19, 2, 3, 9, 1, 4, 5, 7, 2, 4, 6, 3,...\n",
      "3583    [4, 4, 4, 1, 4, 3, 2, 1, 4, 2, 1, 13, 2, 1, 4,...\n",
      "3584    [3, 4, 11, 4, 6, 6, 3, 4, 1, 7, 12, 2, 3, 3, 1...\n",
      "3585    [4, 3, 4, 5, 2, 1, 28, 28, 28, 28, 28, 28, 28,...\n",
      "3586    [6, 3, 2, 3, 3, 1, 3, 3, 2, 2, 1, 6, 3, 5, 4, ...\n",
      "3587    [1, 2, 2, 4, 11, 4, 4, 5, 2, 7, 2, 2, 4, 1, 12...\n",
      "3588    [1, 4, 5, 2, 4, 5, 19, 3, 20, 1, 4, 2, 1, 2, 2...\n",
      "3589    [1, 8, 1, 4, 1, 17, 3, 3, 4, 2, 3, 8, 15, 3, 4...\n",
      "3590    [1, 7, 4, 4, 3, 14, 1, 4, 1, 4, 2, 7, 3, 4, 5,...\n",
      "3591    [4, 1, 2, 2, 3, 4, 3, 3, 4, 5, 2, 13, 4, 3, 5,...\n",
      "3592    [4, 1, 4, 1, 4, 8, 2, 6, 3, 4, 5, 2, 1, 4, 3, ...\n",
      "3593    [4, 3, 7, 4, 14, 1, 4, 6, 6, 3, 5, 5, 2, 2, 1,...\n",
      "3594    [4, 4, 5, 2, 4, 1, 12, 3, 4, 5, 2, 1, 11, 4, 3...\n",
      "3595    [1, 2, 2, 7, 1, 2, 1, 14, 5, 2, 1, 3, 3, 6, 9,...\n",
      "3596    [12, 1, 4, 3, 5, 8, 3, 3, 5, 2, 1, 4, 7, 2, 24...\n",
      "3597    [10, 4, 6, 3, 1, 4, 1, 4, 7, 2, 2, 3, 2, 1, 11...\n",
      "3598    [10, 7, 2, 4, 1, 4, 1, 3, 1, 4, 3, 5, 4, 1, 4,...\n",
      "3599    [3, 1, 3, 3, 2, 1, 2, 4, 1, 6, 3, 2, 6, 2, 10,...\n",
      "Name: pos, Length: 3600, dtype: object\n",
      "0       [6, 2, 3, 1, 1, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "1       [5, 3, 2, 5, 1, 2, 1, 1, 20, 20, 20, 20, 20, 2...\n",
      "2       [2, 1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,...\n",
      "3       [2, 2, 2, 20, 20, 20, 20, 20, 20, 20, 20, 20, ...\n",
      "4       [1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "5       [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "6       [9, 1, 2, 1, 7, 5, 5, 7, 3, 1, 20, 20, 20, 20,...\n",
      "7       [6, 2, 6, 3, 7, 6, 2, 1, 6, 1, 20, 20, 20, 20,...\n",
      "8       [2, 3, 1, 3, 5, 3, 3, 5, 3, 2, 1, 3, 9, 20, 20...\n",
      "9       [4, 3, 6, 20, 20, 20, 20, 20, 20, 20, 20, 20, ...\n",
      "10      [1, 2, 3, 1, 1, 1, 8, 20, 20, 20, 20, 20, 20, ...\n",
      "11      [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "12      [1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "13      [11, 2, 3, 1, 20, 20, 20, 20, 20, 20, 20, 20, ...\n",
      "14      [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "15      [3, 6, 6, 1, 2, 2, 1, 20, 20, 20, 20, 20, 20, ...\n",
      "16      [2, 1, 1, 2, 1, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "17      [4, 2, 3, 4, 2, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "18      [2, 1, 1, 5, 2, 3, 2, 20, 20, 20, 20, 20, 20, ...\n",
      "19      [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "20      [2, 9, 2, 9, 1, 2, 1, 2, 1, 20, 20, 20, 20, 20...\n",
      "21      [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "22      [3, 2, 3, 3, 5, 2, 3, 3, 7, 20, 20, 20, 20, 20...\n",
      "23      [2, 5, 3, 1, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "24      [2, 1, 2, 3, 3, 1, 2, 1, 1, 20, 20, 20, 20, 20...\n",
      "25      [1, 2, 2, 1, 2, 1, 20, 20, 20, 20, 20, 20, 20,...\n",
      "26      [2, 6, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,...\n",
      "27      [1, 4, 1, 6, 2, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "28      [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "29      [2, 1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,...\n",
      "                              ...                        \n",
      "3570    [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "3571    [2, 1, 2, 5, 1, 5, 2, 5, 1, 20, 20, 20, 20, 20...\n",
      "3572    [2, 9, 3, 3, 1, 5, 1, 1, 3, 5, 2, 1, 3, 3, 20,...\n",
      "3573    [2, 1, 2, 1, 5, 1, 1, 20, 20, 20, 20, 20, 20, ...\n",
      "3574    [2, 7, 1, 3, 2, 1, 20, 20, 20, 20, 20, 20, 20,...\n",
      "3575    [2, 3, 1, 3, 3, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "3576    [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "3577    [6, 2, 1, 6, 3, 6, 3, 3, 3, 1, 20, 20, 20, 20,...\n",
      "3578    [2, 3, 1, 2, 1, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "3579    [3, 1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,...\n",
      "3580    [2, 2, 4, 6, 4, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "3581    [2, 2, 4, 6, 1, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "3582    [2, 6, 1, 2, 6, 3, 1, 3, 1, 20, 20, 20, 20, 20...\n",
      "3583    [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "3584    [1, 5, 1, 5, 9, 3, 2, 3, 3, 1, 20, 20, 20, 20,...\n",
      "3585    [2, 1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,...\n",
      "3586    [3, 1, 3, 3, 1, 3, 1, 1, 7, 3, 1, 20, 20, 20, ...\n",
      "3587    [1, 2, 3, 1, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "3588    [1, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "3589    [2, 2, 12, 20, 20, 20, 20, 20, 20, 20, 20, 20,...\n",
      "3590    [2, 2, 1, 1, 4, 2, 10, 1, 20, 20, 20, 20, 20, ...\n",
      "3591    [2, 3, 1, 2, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "3592    [2, 3, 1, 2, 1, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "3593    [2, 2, 3, 1, 20, 20, 20, 20, 20, 20, 20, 20, 2...\n",
      "3594    [2, 1, 5, 2, 1, 2, 1, 20, 20, 20, 20, 20, 20, ...\n",
      "3595    [3, 2, 1, 2, 2, 1, 3, 20, 20, 20, 20, 20, 20, ...\n",
      "3596    [6, 2, 1, 2, 3, 2, 1, 20, 20, 20, 20, 20, 20, ...\n",
      "3597    [4, 2, 3, 2, 6, 1, 2, 1, 20, 20, 20, 20, 20, 2...\n",
      "3598    [2, 1, 2, 20, 20, 20, 20, 20, 20, 20, 20, 20, ...\n",
      "3599    [3, 1, 2, 3, 1, 20, 20, 20, 20, 20, 20, 20, 20...\n",
      "Name: role, Length: 3600, dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-948cfafa09fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'role'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tags'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tags'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'positionE1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'positionE1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_padding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'positionE2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'positionE2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_padding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2549\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2551\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6a10a2079b39>\u001b[0m in \u001b[0;36mposition_padding\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m#位置特征padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mposition_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6a10a2079b39>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m#位置特征padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mposition_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "#用pandas处理数据\n",
    "df_data = pd.DataFrame({'words': datas, 'tags': labels,'positionE1':positionE1,'positionE2':positionE2,'pos':pos_tag,'relation':relation,'role':roles}, index=range(len(datas)))\n",
    "#word和pos这两列的值应用自己定义的X_padding函数\n",
    "df_data['words'] = df_data['words'].apply(X_padding)\n",
    "df_data['pos'] = df_data['pos'].apply(p_padding)\n",
    "df_data['relation'] = df_data['relation'].apply(R_padding)\n",
    "df_data['role'] = df_data['role'].apply(Role_padding)\n",
    "print(df_data['words'])\n",
    "print(df_data['pos'])\n",
    "print(df_data['role'])\n",
    "df_data['tags'] = df_data['tags']\n",
    "df_data['positionE1'] = df_data['positionE1'].apply(position_padding)\n",
    "df_data['positionE2'] = df_data['positionE2'].apply(position_padding)\n",
    "\n",
    "datas = np.asarray(list(df_data['words'].values))#单词\n",
    "labels = np.asarray(list(df_data['tags'].values))#类别标签\n",
    "positionE1 = np.asarray(list(df_data['positionE1'].values))#位置1\n",
    "positionE2 = np.asarray(list(df_data['positionE2'].values))#位置2\n",
    "pos = np.asarray(list(df_data['pos'].values))#单词词性\n",
    "relation = np.asarray(list(df_data['relation'].values))\n",
    "role = np.asarray(list(df_data['role'].values))\n",
    "\n",
    "#开始保存\n",
    "import pickle\n",
    "with open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\people_relation_test.pkl', 'wb') as outp:\n",
    "\tpickle.dump(datas, outp)\n",
    "\tpickle.dump(labels, outp)\n",
    "\tpickle.dump(positionE1, outp)\n",
    "\tpickle.dump(positionE2, outp)\n",
    "\tpickle.dump(pos, outp)\n",
    "\tpickle.dump(relation, outp)\n",
    "\tpickle.dump(role, outp) \n",
    "print ('** Finished saving the test data.') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ow7HLMyG10_6"
   },
   "source": [
    "加载中文预训练词向量400000*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2050
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135631,
     "status": "ok",
     "timestamp": 1555068870124,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "vbWHyrSt1wBT",
    "outputId": "ab88b3b7-5e75-48a6-8ac3-81f855dae4ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2259,  0.1076,  0.1972, -0.1635,  0.0908,  0.0406,  0.1767, -0.0113,\n",
      "         -0.0530,  0.0376, -0.1555,  0.0538,  0.1310,  0.2501, -0.0714, -0.0898,\n",
      "         -0.0342,  0.0786,  0.0239,  0.1597,  0.1004,  0.0218,  0.2663,  0.0043,\n",
      "          0.1060, -0.0028,  0.1198,  0.0042, -0.1542,  0.0880,  0.1791,  0.0417,\n",
      "         -0.1508,  0.1126, -0.0032, -0.1160,  0.0422,  0.1088,  0.1386, -0.2708,\n",
      "          0.2761, -0.3775, -0.1338,  0.2253, -0.0850, -0.0465, -0.1634, -0.1297,\n",
      "          0.1787, -0.0081, -0.0375,  0.2917,  0.1443, -0.1186,  0.0466,  0.0219,\n",
      "          0.1262,  0.0543,  0.0482,  0.0793, -0.1262,  0.0454, -0.0992, -0.0164,\n",
      "         -0.0095, -0.0383, -0.1525,  0.0137, -0.2109, -0.1517,  0.0688,  0.3104,\n",
      "          0.0863,  0.0655,  0.0898,  0.2640,  0.2064, -0.0463,  0.1116, -0.1129,\n",
      "          0.0250,  0.2663,  0.2390, -0.1127,  0.0372, -0.2285,  0.0486,  0.2430,\n",
      "         -0.1435,  0.0450,  0.0282,  0.0966,  0.0110,  0.1193,  0.0684, -0.0002,\n",
      "         -0.0111, -0.0962, -0.0205, -0.1042, -0.1528, -0.1263,  0.0034,  0.1467,\n",
      "          0.0342, -0.0631, -0.1006,  0.0820,  0.2971, -0.0954,  0.0479,  0.0451,\n",
      "          0.0612, -0.1039, -0.0461, -0.1083,  0.0839, -0.1701,  0.0919, -0.1113,\n",
      "          0.0364,  0.0483,  0.0480, -0.1331, -0.1735, -0.0625,  0.1335,  0.2645,\n",
      "         -0.1990, -0.1347, -0.1760, -0.0733, -0.0718, -0.0677,  0.0659, -0.0618,\n",
      "         -0.2079, -0.0357,  0.1291,  0.1606,  0.0642,  0.0361, -0.0376, -0.1237,\n",
      "          0.0702, -0.0116,  0.0955, -0.0261,  0.1768,  0.1353, -0.0916, -0.1963,\n",
      "          0.1358, -0.0673, -0.0660, -0.2077, -0.1789, -0.0094, -0.1139,  0.1966,\n",
      "         -0.1147, -0.0263, -0.1416,  0.1974, -0.0785, -0.1627,  0.0521,  0.0037,\n",
      "          0.0349, -0.0677, -0.0148,  0.0252, -0.0123,  0.0144,  0.0157,  0.0446,\n",
      "          0.0072, -0.0307, -0.0755,  0.1437,  0.0752,  0.1414, -0.0387,  0.1203,\n",
      "          0.0664,  0.0289, -0.0267,  0.0525,  0.1033, -0.0580,  0.0582,  0.0587,\n",
      "         -0.1961, -0.1188, -0.0174,  0.0470,  0.3016,  0.0379, -0.1473,  0.3408,\n",
      "         -0.0155, -0.0044,  0.0090, -0.0365,  0.1710,  0.2241, -0.1198,  0.3025,\n",
      "         -0.0362, -0.2001,  0.1084,  0.0484,  0.0590,  0.0921,  0.0246,  0.0496,\n",
      "         -0.2052,  0.0181, -0.3306,  0.0478, -0.0313, -0.0663, -0.0778,  0.2742,\n",
      "         -0.1575, -0.0903, -0.0571,  0.0991,  0.0941, -0.1523, -0.0126,  0.0656,\n",
      "          0.0321,  0.1229,  0.0515,  0.0197,  0.3214,  0.1003, -0.1954,  0.0336,\n",
      "          0.1719, -0.0550, -0.0905, -0.0460, -0.0232,  0.1421,  0.1604, -0.1002,\n",
      "          0.1142, -0.2511, -0.0209,  0.2599,  0.0108, -0.3331, -0.0298, -0.1067,\n",
      "         -0.0662, -0.0550,  0.0321,  0.0816,  0.2373,  0.0345,  0.1168, -0.0549,\n",
      "          0.0358, -0.1716, -0.0775,  0.0910, -0.0500,  0.0809, -0.3566, -0.0448,\n",
      "         -0.0590,  0.1918,  0.0011,  0.0365, -0.0471, -0.0512,  0.0282,  0.2307,\n",
      "         -0.0932, -0.0864, -0.1532, -0.0006,  0.0284, -0.1173, -0.1547, -0.0302,\n",
      "         -0.0737,  0.0227, -0.0370,  0.0596,  0.1533, -0.1038,  0.2319,  0.2474,\n",
      "         -0.1347,  0.1421,  0.1441,  0.0057]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import gensim\n",
    "\n",
    "vocab_size = len(word2id)+1  \n",
    "embed_size = 300\n",
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300 - 40w.txt',\n",
    "                                                          binary=False,encoding='utf-8')\n",
    "\n",
    "\n",
    "weight = torch.zeros(vocab_size+1, embed_size)\n",
    "\n",
    "for i in range(len(wvmodel.index2word)):\n",
    "    try:\n",
    "        index = word2id[wvmodel.index2word[i]]\n",
    "        #print(\"yes\")\n",
    "    except:\n",
    "        continue    \n",
    "    weight[index, :] = torch.from_numpy(wvmodel.get_vector(id2word[word2id[wvmodel.index2word[i]]]))\n",
    "embedding_pre = weight\n",
    "print(embedding_pre[:2])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGLXZvWY2FTg"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAtt_BLSTM(nn.Module):\n",
    "    def __init__(self,config,embedding_pre):\n",
    "        super(SelfAtt_BLSTM, self).__init__()\n",
    "        \n",
    "        # Set hyper parameters\n",
    "        self.word_emb_dim =config['word_emb_dim']\n",
    "        self.word_vocab = config['word_vocab']#单词数，词表大小\n",
    "        \n",
    "        \n",
    "        self.pos_tag_dim = config['pos_tag_dim']\n",
    "        self.pos_tag_vocab = config['pos_tag_vocab']\n",
    "        \n",
    "        self.relation_dim = config['relation_dim']\n",
    "        self.relation_vocab = config['relation_vocab']\n",
    "        \n",
    "        self.role_dim = config['role_dim']\n",
    "        self.role_vocab = config['role_vocab']\n",
    "        \n",
    "        self.hidden_dim = config['hidden_dim'] #隐藏单元个数\n",
    "        self.output_dim = config['output_dim']#输出维度\n",
    "        \n",
    "    \n",
    "        self.MAX_POS = config['MAX_POS']#位置\n",
    "        self.pos_emb_dim =  config['pos_emb_dim']\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        #embedding layer\n",
    "        self.word_emb = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_pre),freeze=False)\n",
    "        \n",
    "        self.pos_tag_emb = nn.Embedding(self.pos_tag_vocab,self.pos_tag_dim)\n",
    "        \n",
    "        self.relation_emb = nn.Embedding(self.relation_vocab,self.relation_dim)\n",
    "        \n",
    "        self.role_emb = nn.Embedding(self.role_vocab,self.role_dim)\n",
    "        \n",
    "        self.pos1_emb = nn.Embedding(self.MAX_POS*2+1, self.pos_emb_dim)\n",
    "        self.pos1_emb.weight.data.uniform_(-0.0, 0.0)\n",
    "       \n",
    "        self.pos2_emb = nn.Embedding(self.MAX_POS*2+1, self.pos_emb_dim)\n",
    "        self.pos2_emb.weight.data.uniform_(-0.0, 0.0)\n",
    "        \n",
    "        #BLSTM layer\n",
    "        self.rnn = nn.LSTM(self.word_emb_dim + self.pos_tag_dim+relation_dim +self.role_dim+self.pos_emb_dim * 2, hidden_dim,num_layers=1, bidirectional=True, batch_first = True)\n",
    "        \n",
    "        #Self-Attention layer\n",
    "        self.attention_hidden = nn.Linear(hidden_dim * 2, hidden_dim,bias=False)\n",
    "\n",
    "        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        \n",
    "        \n",
    "        #classifier\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim)\n",
    "  \n",
    "    def forward(self, sentence,pos_tag,relation,role,pos1,pos2 ,is_train = True):\n",
    "        #print(\"sentence.shape:\",sentence.shape)\n",
    "        #print(\"pos1.shape:\",pos1.shape)\n",
    "        #print(\"pos2.shape:\",pos2.shape)\n",
    "        word_embeddings = self.word_emb(sentence)\n",
    "        pos_tag_embeddings = self.pos_tag_emb(pos_tag)\n",
    "        relation_embeddings = self.relation_emb(relation)\n",
    "        role_embeddings = self.role_emb(role)\n",
    "        pos1_embeddings = self.pos1_emb(pos1)\n",
    "        pos2_embeddings = self.pos2_emb(pos2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # LSTM layer\n",
    "        X = torch.cat((word_embeddings,pos_tag_embeddings,relation_embeddings,role_embeddings,pos1_embeddings, pos2_embeddings),-1)\n",
    "        #X = word_embeddings\n",
    "        #print(\"word_embeddings:\",X[:2])\n",
    "        #print(\"pos1_embeddings:\",pos1_embeddings[:2])\n",
    "        #print(\"pos2_embeddings:\",pos2_embeddings[:2])\n",
    "       # X = F.dropout(X, p=0.7)#将词嵌入层Droupout下\n",
    "        for_output,hiddens = self.rnn(X)\n",
    "        #print(\"output.shape:\",hiddens.shape)\n",
    "        #rev_hiddens, rev_output = self.rev_lstm(X)\n",
    "        #print(\"word_embeddings.shape\",word_embeddings.shape)\n",
    "        #print(\"hiddens.shape:\", hiddens.shape)\n",
    "        #print(\"pos_tag_embeddings.shape:\", pos_tag_embeddings.shape)\n",
    "        #print(\"relationembeddings.shape:\",relation_embeddings.shape)\n",
    "        #print(\"pos1_embeddings.shape:\",pos1_embeddings.shape)\n",
    "        #print(\"pos2_embeddings.shape:\",pos2_embeddings.shape)\n",
    "        # Self Attentive layer自注意力层\n",
    "        #att_input = torch.cat((for_output,pos_tag_embeddings,relation_embeddings,pos1_embeddings, pos2_embeddings),-1)#拼接\n",
    "        #print(\"att_input.shape:\",att_input.shape)\n",
    "        att_input = F.dropout(for_output, p=0.7)#拼接后的层dropout\n",
    "        \n",
    "        att_hidden = torch.tanh(self.attention_hidden(att_input))#feed自注意力层\n",
    "        att_hidden = F.dropout(att_hidden, p=0.7)\n",
    "        #print(\"att_hidden.shape:\", att_hidden.shape)\n",
    "        #att_hidden = F.dropout(att_hidden, p=0.6)#对att_hiddendropout下\n",
    "        att_scores = self.attention(att_hidden)#得到注意力分数\n",
    "        att_scores = F.dropout(att_scores, p=0.7)\n",
    "        #print(\"att_scores.shape:\",att_scores.shape)\n",
    "        #print(\"att_scores.shape:\", att_scores.shape)\n",
    "        attention_distrib = F.softmax(att_scores, dim = 1)#对分数进行归一化,a=softmax(ws2*tanh(ws1*H.transpose))\n",
    "       # print(\"attention_distrib.shape:\",attention_distrib.shape)\n",
    "        #hiddens = F.dropout(hiddens, p=0.5)#hiddens dropout下\n",
    "        context_vector = torch.sum(for_output * attention_distrib, dim = 1)#得到背景向量context_vector,加权求和\n",
    "        #print(\"context_vector.shape:\",context_vector.shape)\n",
    "        # Classifier\n",
    "        #context_hidden = self.classifier_hidden(context_vector)\n",
    "        #context_hidden = F.dropout(context_hidden, p=0.5, training=is_train)\n",
    "        finals = self.classifier(context_vector)\n",
    "        #finals = F.softmax(self.classifier(context_vector), dim = 1)#将背景向量送入分类器进行分类\n",
    "       # print(\"finals.shape\",finals.shape)\n",
    "        return finals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6371,
     "status": "ok",
     "timestamp": 1555069087071,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "APFtU_wI2DTn",
    "outputId": "40069680-90ca-41ba-f89b-065e7d6d901a"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'container_abcs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-56e7f3e3d4e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#加载保存好的训练dataset.pkl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistributedSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_six\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'container_abcs'"
     ]
    }
   ],
   "source": [
    "#coding:utf8\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as D\n",
    "\n",
    "#加载保存好的训练dataset.pkl\n",
    "with open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\people_relation_train.pkl', 'rb') as inp:\n",
    "    word2id = pickle.load(inp)\n",
    "    print(\"word2id.shape:\",word2id.shape)\n",
    "    id2word = pickle.load(inp)\n",
    "    postag2id = pickle.load(inp)\n",
    "    Relation2id = pickle.load(inp)\n",
    "    print(\"Relation2id.shape:\",Relation2id)\n",
    "    train = pickle.load(inp)\n",
    "    \n",
    "    labels = pickle.load(inp)\n",
    "    print(\"labels.shape:\",labels.shape)\n",
    "    position1 = pickle.load(inp)\n",
    "    print(\"position1.shape:\",position1.shape)\n",
    "    position2 = pickle.load(inp)\n",
    "    print(\"position2.shape:\",position2.shape)\n",
    "    pos_tag = pickle.load(inp)\n",
    "    relation = pickle.load(inp)\n",
    "    role = pickle.load(inp)\n",
    "    print(\"relation.shape\",relation.shape)\n",
    "#10折验证取两万个样本\n",
    "\n",
    "train = train[:20000]  \n",
    "labels = labels[:20000]\n",
    "position1 =  position1[:20000]\n",
    "position2 =  position2[:20000]  \n",
    "postag = pos_tag[:20000]\n",
    "Relation = relation[:20000]\n",
    "#print(train[:2])\n",
    "\n",
    "\n",
    "#加载保存好的测试dataset.pkl    \n",
    "with open('E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\data\\people_relation_test.pkl', 'rb') as inp:\n",
    "    test = pickle.load(inp)\n",
    "    labels_t = pickle.load(inp)\n",
    "    position1_t = pickle.load(inp)\n",
    "    position2_t = pickle.load(inp)\n",
    "    pos_tag_t = pickle.load(inp)\n",
    "    relation_t = pickle.load(inp)\n",
    "    role_t = pickle.load(inp)\n",
    "\"\"\"\n",
    "test =test[:2500]\n",
    "labels_t = labels_t[:2500]\n",
    "position1_t = position1_t[:2500]\n",
    "position2_t = position2_t[:2500]\n",
    "pos_tag_t = pos_tag_t[:2500]\n",
    "relation_t = relation_t[:2500]\n",
    "\"\"\"\n",
    "print (\"train len\", len(train))#训练样本数     \n",
    "print (\"test len\", len(test)) #测试样本数  \n",
    "print (\"word2id len\",len(word2id))\n",
    "print(\"test len\",len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11693
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3939847,
     "status": "ok",
     "timestamp": 1552895493585,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "tfudsD20lMec",
    "outputId": "2881416a-a77e-4828-f2fd-8189ee8f0102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "SelfAtt_BLSTM(\n",
      "  (word_emb): Embedding(39770, 300)\n",
      "  (pos_tag_emb): Embedding(30, 30)\n",
      "  (relation_emb): Embedding(82, 30)\n",
      "  (role_emb): Embedding(82, 30)\n",
      "  (pos1_emb): Embedding(165, 30)\n",
      "  (pos2_emb): Embedding(165, 30)\n",
      "  (rnn): LSTM(450, 200, batch_first=True, bidirectional=True)\n",
      "  (attention_hidden): Linear(in_features=400, out_features=200, bias=False)\n",
      "  (attention): Linear(in_features=200, out_features=1, bias=False)\n",
      "  (classifier): Linear(in_features=400, out_features=12, bias=True)\n",
      ")\n",
      "**************************\n",
      "self._name:  cudart64_80\n",
      "**************************\n",
      "**************************\n",
      "self._name:  cudnn64_7\n",
      "**************************\n",
      "train.shape: torch.Size([19998, 50])\n",
      "train_datasets: torch.Size([50])\n",
      "epoch: 0\n",
      "train: 55.590559055905594 %\n",
      "train_loss: 0.2250144\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data as D\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np\n",
    "        \n",
    "\n",
    "#BiLSTM_SelfATT模型训练\n",
    "#BiLSTM_SelfATT参数设置\n",
    "word_vocab=len(word2id)+1   \n",
    "word_emb_dim=300\n",
    "\n",
    "\n",
    "\n",
    "MAX_POS = 82#不同数据集这里可能会报错。这里的pos_size一定要比数据集里的序列长度大，不然会报错\n",
    "pos_emb_dim=30\n",
    "\n",
    "pos_tag_dim = 30\n",
    "pos_tag_vocab = len(postag2id)+1\n",
    "\n",
    "relation_dim = 30\n",
    "relation_vocab = 82\n",
    "\n",
    "role_dim = 30\n",
    "role_vocab = 82\n",
    "\n",
    "hidden_dim=200\n",
    "\n",
    "output_dim = len(Relation2id)#关系类别\n",
    "print(output_dim)\n",
    "\n",
    "BATCH = 6\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "\n",
    "config={}\n",
    "config['word_vocab'] = word_vocab\n",
    "config['word_emb_dim'] = word_emb_dim\n",
    "\n",
    "config['pos_tag_dim'] = pos_tag_dim\n",
    "config['pos_tag_vocab'] = pos_tag_vocab\n",
    "\n",
    "config['relation_dim'] = relation_dim\n",
    "config['relation_vocab'] = relation_vocab\n",
    "\n",
    "config['role_dim'] = role_dim\n",
    "config['role_vocab'] = role_vocab\n",
    "\n",
    "config['MAX_POS'] = MAX_POS\n",
    "config['pos_emb_dim'] = pos_emb_dim\n",
    "\n",
    "config['hidden_dim'] = hidden_dim\n",
    "config['output_dim'] = output_dim\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    " #0.0005\n",
    "device= torch.device('cuda:0')\n",
    "\n",
    "model = SelfAtt_BLSTM(config,embedding_pre)\n",
    "print(model)\n",
    "params = list(model.parameters())\n",
    "#model.to(device)\n",
    "model = model.to(device)\n",
    "#model = torch.load('model/model_epoch20.pkl')\n",
    "\n",
    "#优化器\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "#optimizer = optim.Adamax(model.parameters(), lr=learning_rate,weight_decay=5e-4 )\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-8 )\n",
    "torch.nn.utils.clip_grad_norm_(params, 10)\n",
    "\n",
    "#损失函数\n",
    "#criterion = nn.CrossEntropyLoss(size_average=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#将数据转成LongTensor格式\n",
    "train = torch.LongTensor(train[:len(train)-len(train)%BATCH])\n",
    "print(\"train.shape:\",train.shape)\n",
    "#pos_tag = torch.LongTensor(pos_tag)\n",
    "pos_tag = torch.LongTensor(pos_tag[:len(train)-len(train)%BATCH])\n",
    "relation = torch.LongTensor(relation[:len(train)-len(train)%BATCH])\n",
    "role = torch.LongTensor(role[:len(train)-len(train)%BATCH])\n",
    "#position1 = torch.LongTensor(position1)\n",
    "position1 = torch.LongTensor(position1[:len(train)-len(train)%BATCH])\n",
    "position2 = torch.LongTensor(position2[:len(train)-len(train)%BATCH])\n",
    "labels = torch.LongTensor(labels[:len(train)-len(train)%BATCH])\n",
    "\n",
    "train_datasets = D.TensorDataset(train,pos_tag,relation,role,position1,position2,labels)\n",
    "train_dataloader = D.DataLoader(train_datasets,BATCH,True,num_workers=2)\n",
    "print(\"train_datasets:\",train_datasets[0][0].shape)\n",
    "\n",
    "\n",
    "#同上\n",
    "test = torch.LongTensor(test[:len(test)-len(test)%BATCH])\n",
    "pos_tag_t = torch.LongTensor(pos_tag_t[:len(test)-len(test)%BATCH])\n",
    "position1_t = torch.LongTensor(position1_t[:len(test)-len(test)%BATCH])\n",
    "relation_t =torch.LongTensor(relation_t[:len(test)-len(test)%BATCH])\n",
    "role_t =torch.LongTensor(role_t[:len(test)-len(test)%BATCH])\n",
    "#position1_t = torch.LongTensor(position1_t)\n",
    "position2_t = torch.LongTensor(position2_t[:len(test)-len(test)%BATCH])\n",
    "labels_t = torch.LongTensor(labels_t[:len(test)-len(test)%BATCH])\n",
    "\n",
    "test_datasets = D.TensorDataset(test,pos_tag_t,relation_t,role_t,position1_t,position2_t,labels_t)\n",
    "test_dataloader = D.DataLoader(test_datasets,BATCH,True,num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "#start training\n",
    "for epoch in range(EPOCHS):\n",
    "    print (\"epoch:\",epoch)\n",
    "    #统计预测正确的标签数\n",
    "    acc=0\n",
    "    #统计预测所有标签的总数\n",
    "    total=0\n",
    "    total_loss=0\n",
    "    \n",
    "    for sentence,pos_tag,relation,role,pos1,pos2,tag in train_dataloader:#批训练\n",
    "          #print(\"sentence:\",sentence)\n",
    "          #print(\"pos1:\",pos1)\n",
    "          #print(\"pos2:\",pos2)\n",
    "\n",
    "          sentence = Variable(sentence.cuda())\n",
    "          pos_tag = Variable(pos_tag.cuda())\n",
    "          relation = Variable(relation.cuda())\n",
    "          role = Variable(role.cuda())\n",
    "          pos1 = Variable(pos1.cuda())\n",
    "          pos2 = Variable(pos2.cuda())\n",
    "          tags = Variable(tag.cuda())\n",
    "          \"\"\"\n",
    "          sentence = Variable(sentence)\n",
    "          pos_tag = Variable(pos_tag)\n",
    "          e1_head = Variable(relation)\n",
    "          pos1 = Variable(pos1)\n",
    "          pos2 = Variable(pos2)\n",
    "          tags = Variable(tag)\n",
    "          \"\"\"\n",
    "          y = model(sentence,pos_tag,relation,role,pos1,pos2)\n",
    "         \n",
    "     \n",
    "          #epoch_loss += loss.data.cpu().numpy()[0]\n",
    "          loss = criterion(y, tags)\n",
    "          total_loss += loss\n",
    "          #loss += l2_loss(params[1:]) * l2_rate\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()    \n",
    "          y = np.argmax(y.data.cpu().numpy(),axis=1)\n",
    "         # print(\"predict:\",y)\n",
    "         # print(\"true：\",tags)   \n",
    "          #y = np.argmax(y.data.numpy(),axis=1)\n",
    "          #print(\"train_y:\",y)\n",
    "          #print(\"train_tag\",tag)\n",
    "          for y1,y2 in zip(y,tag):\n",
    "             # print(\"train_y1:\",y1)\n",
    "             # print(\"train_y2:\",y2)\n",
    "              if y1==y2:\n",
    "                  acc+=1\n",
    "\n",
    "              total+=1\n",
    "      \n",
    "    print (\"train:\",100*float(acc)/total,\"%\")\n",
    "    print(\"train_loss:\",(total_loss/total).data.cpu().numpy())\n",
    "     # print((total_loss/len(train_dataloader.dataset)).cpu().data.numpy())\n",
    "      #print((total_loss/len(train_dataloader.dataset)).data.numpy())\n",
    "      #print(loss.cpu().data.numpy()) \n",
    "\n",
    "    \n",
    "    \n",
    "    #start testing\n",
    "    acc_t=0\n",
    "    total_t=0\n",
    "    total_loss_t=0\n",
    "    count_predict = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    count_total = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    count_right = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for sentence,pos_tag,relation,role,pos1,pos2,tag in test_dataloader:  \n",
    "\n",
    "          sentence = Variable(sentence.cuda()) \n",
    "          pos_tag = Variable(pos_tag.cuda()) \n",
    "          relation = Variable(relation.cuda())\n",
    "          role = Variable(role.cuda())\n",
    "          pos1 = Variable(pos1.cuda())\n",
    "          pos2 = Variable(pos2.cuda())\n",
    "          tags = Variable(tag.cuda())\n",
    "          \"\"\"\n",
    "          sentence = Variable(sentence)\n",
    "          pos_tag = Variable(pos_tag)\n",
    "          relation = Variable(relation)\n",
    "          pos1 = Variable(pos1)\n",
    "          pos2 = Variable(pos2)\n",
    "          tag = Variable(tag)\n",
    "           \"\"\"\n",
    "          y = model(sentence,pos_tag,relation,relation,role,pos1,pos2)\n",
    "          loss = criterion(y, tags)\n",
    "          total_loss_t += loss\n",
    "\n",
    "          y = np.argmax(y.data.cpu().numpy(),axis=1)\n",
    "         # y = np.argmax(y.data.numpy(),axis=1)\n",
    "         # print('testy:',y)\n",
    "         # print('testtag:',tag)\n",
    "          for y1,y2 in zip(y,tag):\n",
    "             # print(\"test_y1:\",y1)\n",
    "              #print(\"test_y2:\",y2)\n",
    "              count_predict[y1]+=1\n",
    "              count_total[y2]+=1\n",
    "              if y1==y2:\n",
    "                  count_right[y1]+=1\n",
    "              total_t+=1\n",
    "\n",
    "    print(\"test_loss:\",(total_loss_t/total_t).data.cpu().numpy())\n",
    "    precision = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    recall = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(count_predict)):\n",
    "          if count_predict[i]!=0 :\n",
    "              precision[i] = float(count_right[i])/count_predict[i]\n",
    "\n",
    "          if count_total[i]!=0:\n",
    "              recall[i] = float(count_right[i])/count_total[i]    \n",
    "\n",
    "    precision = sum(precision)/len(Relation2id)\n",
    "    recall = sum(recall)/len(Relation2id)\n",
    "    print (\"准确率：\",precision)\n",
    "    print (\"召回率：\",recall)\n",
    "    print (\"f：\", (2*precision*recall)/(precision+recall))\n",
    "\n",
    "#start saveing\n",
    "torch.save(model, \"E:\\litao\\Relation_Extraction\\SelfAtt_BLSTM_CH\\model\\model_BiLSTM_selfAtt_ch.pkl\")\n",
    "print (\"model has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dot(var, params=None):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '(' + (', ').join(['%d' % v for v in size]) + ')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot\n",
    "if __name__ == '__main__':\n",
    "    net = model\n",
    "    X = torch.cat((train,pos_tag,relation,role,position1,position2,),-1) \n",
    "    y = net(X)\n",
    "    g = make_dot(y)\n",
    "    g.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7DE9a7MB6K0"
   },
   "outputs": [],
   "source": [
    "#使用grid search 来调参\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data as D\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np\n",
    "        \n",
    "\n",
    "#BiLSTM_SelfATT模型训练\n",
    "#BiLSTM_SelfATT参数设置\n",
    "word_vocab=len(word2id)+1   \n",
    "word_emb_dim=300\n",
    "\n",
    "\n",
    "\n",
    "MAX_POS = 82#不同数据集这里可能会报错。这里的pos_size一定要比数据集里的序列长度大，不然会报错\n",
    "pos_emb_dim=25\n",
    "\n",
    "pos_tag_dim = 30\n",
    "pos_tag_vocab = len(postag2id)+1\n",
    "\n",
    "relation_dim = 30\n",
    "relation_vocab = 82\n",
    "\n",
    "\n",
    "hidden_dim=150\n",
    "\n",
    "output_dim = len(Relation2id)#关系类别\n",
    "print(output_dim)\n",
    "\n",
    "BATCH = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "l2_rate = 0.3\n",
    "\n",
    "config={}\n",
    "config['word_vocab'] = word_vocab\n",
    "config['word_emb_dim'] = word_emb_dim\n",
    "\n",
    "config['pos_tag_dim'] = pos_tag_dim\n",
    "config['pos_tag_vocab'] = pos_tag_vocab\n",
    "\n",
    "config['relation_dim'] = relation_dim\n",
    "config['relation_vocab'] = relation_vocab\n",
    "\n",
    "config['MAX_POS'] = MAX_POS\n",
    "config['pos_emb_dim'] = pos_emb_dim\n",
    "\n",
    "config['hidden_dim'] = hidden_dim\n",
    "config['output_dim'] = output_dim\n",
    "\n",
    "\n",
    "#将数据转成LongTensor格式\n",
    "train = torch.LongTensor(train[:len(train)-len(train)%BATCH])\n",
    "print(\"train.shape:\",train.shape)\n",
    "#pos_tag = torch.LongTensor(pos_tag)\n",
    "pos_tag = torch.LongTensor(pos_tag[:len(train)-len(train)%BATCH])\n",
    "relation = torch.LongTensor(relation[:len(train)-len(train)%BATCH])\n",
    "#position1 = torch.LongTensor(position1)\n",
    "position1 = torch.LongTensor(position1[:len(train)-len(train)%BATCH])\n",
    "position2 = torch.LongTensor(position2[:len(train)-len(train)%BATCH])\n",
    "labels = torch.LongTensor(labels[:len(train)-len(train)%BATCH])\n",
    "\n",
    "train_datasets = D.TensorDataset(train,pos_tag,relation,position1,position2,labels)\n",
    "train_dataloader = D.DataLoader(train_datasets,BATCH,True,num_workers=2)\n",
    "print(\"train_datasets:\",train_datasets[0][0].shape)\n",
    "\n",
    "\n",
    "#同上\n",
    "test = torch.LongTensor(test[:len(test)-len(test)%BATCH])\n",
    "pos_tag_t = torch.LongTensor(pos_tag_t[:len(test)-len(test)%BATCH])\n",
    "position1_t = torch.LongTensor(position1_t[:len(test)-len(test)%BATCH])\n",
    "relation_t =torch.LongTensor(relation_t[:len(test)-len(test)%BATCH])\n",
    "#position1_t = torch.LongTensor(position1_t)\n",
    "position2_t = torch.LongTensor(position2_t[:len(test)-len(test)%BATCH])\n",
    "labels_t = torch.LongTensor(labels_t[:len(test)-len(test)%BATCH])\n",
    "\n",
    "test_datasets = D.TensorDataset(test,pos_tag_t,relation_t,position1_t,position2_t,labels_t)\n",
    "test_dataloader = D.DataLoader(test_datasets,BATCH,True,num_workers=2)\n",
    "\n",
    "\n",
    "f = 0.83\n",
    "#learning_rate =3e-4\n",
    "\n",
    "# 0.0005\n",
    "device= torch.device('cuda:0')\n",
    "\n",
    "model = Baseline_Model(config,embedding_pre)\n",
    "print(model)\n",
    "params = list(model.parameters())\n",
    "#model.to(device)\n",
    "model = model.cuda()\n",
    "#model = torch.load('model/model_epoch20.pkl')\n",
    "\n",
    "#损失函数\n",
    "#criterion = nn.CrossEntropyLoss(size_average=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#优化器\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum = 0.9, weight_decay=5e-4)\n",
    "#optimizer = optim.Adamax(model.parameters(), lr=learning_rate,weight_decay=5e-4 )\n",
    "#start training\n",
    "for learning_rate in [0.1,0.01,0.5,0.001,0.005,0.0005]:\n",
    "    for w in  [1e-8 ,5e-4,0.0001,0.0006,0.01,1]:\n",
    "        for c in [1,2,3,4,5,6,9,10]:\n",
    "          optimizer = optim.Adam(params, lr=learning_rate,weight_decay=w)\n",
    "          torch.nn.utils.clip_grad_norm_(params, c)\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "         \n",
    "          for epoch in range(EPOCHS):\n",
    "              print (\"epoch:\",epoch)\n",
    "              #统计预测正确的标签数\n",
    "              acc=0\n",
    "              #统计预测所有标签的总数\n",
    "              total=0\n",
    "              total_loss=0\n",
    "\n",
    "              for sentence,pos_tag,relation,pos1,pos2,tag in train_dataloader:#批训练\n",
    "                    #print(\"sentence:\",sentence)\n",
    "                    #print(\"pos1:\",pos1)\n",
    "                    #print(\"pos2:\",pos2)\n",
    "\n",
    "                    sentence = Variable(sentence.cuda())\n",
    "                    pos_tag = Variable(pos_tag.cuda())\n",
    "                    relation = Variable(relation.cuda())\n",
    "                    pos1 = Variable(pos1.cuda())\n",
    "                    pos2 = Variable(pos2.cuda())\n",
    "                    tags = Variable(tag.cuda())\n",
    "                    \"\"\"\n",
    "                    sentence = Variable(sentence)\n",
    "                    pos_tag = Variable(pos_tag)\n",
    "                    e1_head = Variable(relation)\n",
    "                    pos1 = Variable(pos1)\n",
    "                    pos2 = Variable(pos2)\n",
    "                    tags = Variable(tag)\n",
    "                    \"\"\"\n",
    "                    y = model(sentence,pos_tag,relation,pos1,pos2)  \n",
    "                    #epoch_loss += loss.data.cpu().numpy()[0]\n",
    "                    loss = criterion(y, tags)\n",
    "                    total_loss += loss \n",
    "                    #loss += l2_loss(params[1:]) * l2_rate\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()    \n",
    "                    y = np.argmax(y.data.cpu().numpy(),axis=1)\n",
    "                    #y = np.argmax(y.data.numpy(),axis=1)\n",
    "                    #print(\"train_y:\",y)\n",
    "                    #print(\"train_tag\",tag)\n",
    "                    for y1,y2 in zip(y,tag):\n",
    "                       # print(\"train_y1:\",y1)\n",
    "                       # print(\"train_y2:\",y2)\n",
    "                        if y1==y2:\n",
    "                            acc+=1\n",
    "\n",
    "                        total+=1\n",
    "\n",
    "              print (\"train:\",100*float(acc)/total,\"%\")\n",
    "              print(\"train_loss:\",(total_loss/total).data.cpu().numpy())\n",
    "               # print((total_loss/len(train_dataloader.dataset)).cpu().data.numpy())\n",
    "                #print((total_loss/len(train_dataloader.dataset)).data.numpy())\n",
    "                #print(loss.cpu().data.numpy()) \n",
    "\n",
    "\n",
    "\n",
    "              #start testing\n",
    "              acc_t=0\n",
    "              total_t=0\n",
    "              total_loss_t=0\n",
    "              count_predict = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "              count_total = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "              count_right = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "              for sentence,pos_tag,relation,pos1,pos2,tag in test_dataloader:  \n",
    "\n",
    "                    sentence = Variable(sentence.cuda()) \n",
    "                    pos_tag = Variable(pos_tag.cuda()) \n",
    "                    relation = Variable(relation.cuda())\n",
    "                    pos1 = Variable(pos1.cuda())\n",
    "                    pos2 = Variable(pos2.cuda())\n",
    "                    tags = Variable(tag.cuda())\n",
    "                    \"\"\"\n",
    "                    sentence = Variable(sentence)\n",
    "                    pos_tag = Variable(pos_tag)\n",
    "                    relation = Variable(relation)\n",
    "                    pos1 = Variable(pos1)\n",
    "                    pos2 = Variable(pos2)\n",
    "                    tag = Variable(tag)\n",
    "                     \"\"\"\n",
    "                    y = model(sentence,pos_tag,relation,relation,pos1,pos2)\n",
    "                    loss = criterion(y, tags)\n",
    "                    total_loss_t += loss \n",
    "\n",
    "                    y = np.argmax(y.data.cpu().numpy(),axis=1)\n",
    "                   # y = np.argmax(y.data.numpy(),axis=1)\n",
    "                   # print('testy:',y)\n",
    "                   # print('testtag:',tag)\n",
    "                    for y1,y2 in zip(y,tag):\n",
    "                       # print(\"test_y1:\",y1)\n",
    "                        #print(\"test_y2:\",y2)\n",
    "                        count_predict[y1]+=1\n",
    "                        count_total[y2]+=1\n",
    "                        if y1==y2:\n",
    "                            count_right[y1]+=1\n",
    "                        total_t+=1\n",
    "\n",
    "              print(\"test_loss:\",(total_loss_t/total_t).data.cpu().numpy())\n",
    "              precision = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "              recall = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "              for i in range(len(count_predict)):\n",
    "                    if count_predict[i]!=0 :\n",
    "                        precision[i] = float(count_right[i])/count_predict[i]\n",
    "\n",
    "                    if count_total[i]!=0:\n",
    "                        recall[i] = float(count_right[i])/count_total[i]    \n",
    "\n",
    "              precision = sum(precision)/len(Relation2id)\n",
    "              recall = sum(recall)/len(Relation2id)\n",
    "              F = (2*precision*recall)/(precision+recall)\n",
    "              print (\"准确率：\",precision)\n",
    "              print (\"召回率：\",recall)\n",
    "              print(\"F1：\", F)\n",
    "              if F>f:\n",
    "                 print('lr:',learning_rate)\n",
    "                 print('weight_decay:',w)\n",
    "                 print('clip_grad:',c)\n",
    "                 \n",
    "              \n",
    "#start saveing\n",
    "torch.save(model, \"drive/论文/ChineseNRE-master更新后/data/model_BiLSTM_selfAtt_ch.pkl\")\n",
    "print (\"model has been saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNgU1cfR2dBI"
   },
   "source": [
    "training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2706,
     "status": "error",
     "timestamp": 1551953195898,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "aueoEcbg2Um4",
    "outputId": "f6d6694a-4552-437c-e8fd-60775f185d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Baseline_Model(\n",
      "  (word_emb): Embedding(39770, 300)\n",
      "  (pos_tag_emb): Embedding(30, 30)\n",
      "  (relation_emb): Embedding(82, 30)\n",
      "  (pos1_emb): Embedding(165, 25)\n",
      "  (pos2_emb): Embedding(165, 25)\n",
      "  (rnn): LSTM(300, 200, batch_first=True, bidirectional=True)\n",
      "  (attention_hidden): Linear(in_features=510, out_features=200, bias=False)\n",
      "  (attention): Linear(in_features=200, out_features=1, bias=False)\n",
      "  (classifier): Linear(in_features=400, out_features=12, bias=True)\n",
      ")\n",
      "epoch: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-343f9653e701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpostag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRelation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposition2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcc_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mpostag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mRelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRelation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "#10折交叉验证\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data as D\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np\n",
    "        \n",
    "\n",
    "#BiLSTM_SelfATT模型训练\n",
    "#BiLSTM_SelfATT参数设置\n",
    "word_vocab=len(word2id)+1   \n",
    "word_emb_dim=300\n",
    "\n",
    "MAX_POS = 82#不同数据集这里可能会报错。这里的pos_size一定要比数据集里的序列长度大，不然会报错\n",
    "pos_emb_dim=25\n",
    "\n",
    "pos_tag_dim = 30\n",
    "pos_tag_vocab = len(postag2id)+1\n",
    "\n",
    "relation_dim = 30#依存关系\n",
    "relation_vocab = 82\n",
    "\n",
    "\n",
    "hidden_dim=200\n",
    "\n",
    "output_dim = len(Relation2id)#关系类别\n",
    "print(output_dim)\n",
    "\n",
    "BATCH = 16\n",
    "EPOCHS = 10\n",
    "\n",
    "l2_rate = 0.3\n",
    "\n",
    "config={}\n",
    "config['word_vocab'] = word_vocab\n",
    "config['word_emb_dim'] = word_emb_dim\n",
    "\n",
    "config['pos_tag_dim'] = pos_tag_dim\n",
    "config['pos_tag_vocab'] = pos_tag_vocab\n",
    "\n",
    "config['relation_dim'] = relation_dim\n",
    "config['relation_vocab'] = relation_vocab\n",
    "\n",
    "config['MAX_POS'] = MAX_POS\n",
    "config['pos_emb_dim'] = pos_emb_dim\n",
    "\n",
    "config['hidden_dim'] = hidden_dim\n",
    "config['output_dim'] = output_dim\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "\n",
    "device= torch.device('cuda:0')\n",
    "\n",
    "model = Baseline_Model(config,embedding_pre)\n",
    "print(model)\n",
    "params = list(model.parameters())\n",
    "#model.to(device)\n",
    "model = model.cuda()\n",
    "#model = torch.load('model/model_epoch20.pkl')\n",
    "\n",
    "#优化器\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum = 0.9, weight_decay=5e-4)\n",
    "#optimizer = optim.Adamax(model.parameters(), lr=learning_rate,weight_decay=5e-4 )\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=5e-4 )\n",
    "#torch.nn.utils.clip_grad_norm_(params, 5.0)\n",
    "\n",
    "#损失函数\n",
    "#criterion = nn.CrossEntropyLoss(size_average=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\"\"\"\n",
    "#将数据转成LongTensor格式\n",
    "train = torch.LongTensor(train[:len(train)-len(train)%BATCH])\n",
    "print(\"train.shape:\",train.shape)\n",
    "#pos_tag = torch.LongTensor(pos_tag)\n",
    "pos_tag = torch.LongTensor(pos_tag[:len(train)-len(train)%BATCH])\n",
    "relation = torch.LongTensor(relation[:len(train)-len(train)%BATCH])\n",
    "#position1 = torch.LongTensor(position1)\n",
    "position1 = torch.LongTensor(position1[:len(train)-len(train)%BATCH])\n",
    "position2 = torch.LongTensor(position2[:len(train)-len(train)%BATCH])\n",
    "labels = torch.LongTensor(labels[:len(train)-len(train)%BATCH])\n",
    "\n",
    "train_datasets = D.TensorDataset(train,pos_tag,relation,position1,position2,labels)\n",
    "train_dataloader = D.DataLoader(train_datasets,BATCH,True,num_workers=2)\n",
    "print(\"train_datasets:\",train_datasets[0][0].shape)\n",
    "\n",
    "\n",
    "#同上\n",
    "test = torch.LongTensor(test[:len(test)-len(test)%BATCH])\n",
    "pos_tag_t = torch.LongTensor(pos_tag_t[:len(test)-len(test)%BATCH])\n",
    "position1_t = torch.LongTensor(position1_t[:len(test)-len(test)%BATCH])\n",
    "relation_t =torch.LongTensor(relation_t[:len(test)-len(test)%BATCH])\n",
    "#position1_t = torch.LongTensor(position1_t)\n",
    "position2_t = torch.LongTensor(position2_t[:len(test)-len(test)%BATCH])\n",
    "labels_t = torch.LongTensor(labels_t[:len(test)-len(test)%BATCH])\n",
    "\n",
    "test_datasets = D.TensorDataset(test,pos_tag_t,relation_t,position1_t,position2_t,labels_t)\n",
    "test_dataloader = D.DataLoader(test_datasets,BATCH,True,num_workers=2)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#start training\n",
    "for epoch in range(EPOCHS):\n",
    "    print (\"epoch:\",epoch)\n",
    "    #统计预测正确的标签数\n",
    "    acc=0\n",
    "    #统计预测所有标签的总数\n",
    "    total=0\n",
    "    total_loss=0\n",
    "    e = 0\n",
    "    train_p = []\n",
    "    test_p = []\n",
    "    test_r = []\n",
    "    test_f = []\n",
    "    step = 2000\n",
    "    #对齐打乱数据 \n",
    "    cc_pos = list(zip(train,postag,Relation,position1 ,position2,labels))\n",
    "    random.shuffle(cc_pos)\n",
    "\n",
    "    train,postag,Relation, position1,position2, labels = zip(*cc_pos)\n",
    "    train = np.asarray(train)\n",
    "    postag = np.asarray(postag)\n",
    "    Relation = np.asarray(Relation)\n",
    "    position1 = np.asarray(position1)\n",
    "    position2 = np.asarray(position2)\n",
    "    labels = np.asarray(labels)\n",
    "    for i in range(10):\n",
    "      #print(train[:e].shape)\n",
    "      #print(train[e+step:])\n",
    "      train_9 = np.concatenate((train[:e],train[e+step:]),0) \n",
    "      #print(\"train_9.shape:\",train_9.shape)\n",
    "      pos_tag_9 = np.concatenate((postag[:e],postag[e+step:]),0)\n",
    "      #print(\"pos_tag_9.shape:\",pos_tag_9.shape)\n",
    "      relation_9 = np.concatenate((Relation[:e],Relation[e+step:]),0)\n",
    "      position1_9 = np.concatenate((position1[:e],position1[e+step:]),0)\n",
    "      position2_9 = np.concatenate((position2[:e],position2[e+step:]),0)\n",
    "      labels_9 = np.concatenate((labels[:e],labels[e+step:]),0)\n",
    "      \n",
    "     \n",
    "      test = train[e:e+step]\n",
    "      #print(len(test))\n",
    "      pos_tag_t = postag[e:e+step]\n",
    "      relation_t = Relation[e:e+step]\n",
    "      position1_t = position1[e:e+step]\n",
    "      position2_t = position2[e:e+step]\n",
    "      labels_t = labels[e:e+step]\n",
    "      #s += step\n",
    "      e += step\n",
    "      #将数据转成LongTensor格式\n",
    "      \n",
    "      train_9 = torch.LongTensor(train_9[:len(train_9)-len(train_9)%BATCH])\n",
    "      #print(\"train_9.shape:\",train_9.shape)\n",
    "      pos_tag_9 = torch.LongTensor(pos_tag_9[:len(train_9)-len(train_9)%BATCH])\n",
    "      relation_9 = torch.LongTensor(relation_9[:len(train_9)-len(train_9)%BATCH])\n",
    "      position1_9 = torch.LongTensor(position1_9[:len(train_9)-len(train_9)%BATCH])\n",
    "      position2_9 = torch.LongTensor(position2_9[:len(train)-len(train)%BATCH])\n",
    "      labels_9 = torch.LongTensor(labels[:len(train_9)-len(train_9)%BATCH])\n",
    "      train_datasets = D.TensorDataset(train_9,pos_tag_9,relation_9,position1_9,position2_9,labels_9)\n",
    "      train_dataloader = D.DataLoader(train_datasets,BATCH,True,num_workers=2)\n",
    "      \n",
    "      test = torch.LongTensor(test[:len(test)-len(test)%BATCH])\n",
    "      pos_tag_t = torch.LongTensor(pos_tag_t[:len(test)-len(test)%BATCH])\n",
    "      relation_t =torch.LongTensor(relation_t[:len(test)-len(test)%BATCH])\n",
    "      position1_t = torch.LongTensor(position1_t[:len(test)-len(test)%BATCH])\n",
    "      #position1_t = torch.LongTensor(position1_t)\n",
    "      position2_t = torch.LongTensor(position2_t[:len(test)-len(test)%BATCH])\n",
    "      labels_t = torch.LongTensor(labels_t[:len(test)-len(test)%BATCH])\n",
    "\n",
    "      test_datasets = D.TensorDataset(test,pos_tag_t,relation_t,position1_t,position2_t,labels_t)\n",
    "      test_dataloader = D.DataLoader(test_datasets,BATCH,True,num_workers=2)\n",
    "      #training\n",
    "      for sentence,pos_tag,relation,pos1,pos2,tag in train_dataloader:#批训练\n",
    "          #print(\"sentence:\",sentence)\n",
    "          #print(\"pos1:\",pos1)\n",
    "          #print(\"pos2:\",pos2)\n",
    "\n",
    "          sentence = Variable(sentence.cuda())\n",
    "          pos_tag = Variable(pos_tag.cuda())\n",
    "          relation = Variable(relation.cuda())\n",
    "          pos1 = Variable(pos1.cuda())\n",
    "          pos2 = Variable(pos2.cuda())\n",
    "          tags = Variable(tag.cuda())\n",
    "          \"\"\"\n",
    "          sentence = Variable(sentence)\n",
    "          pos_tag = Variable(pos_tag)\n",
    "          e1_head = Variable(relation)\n",
    "          pos1 = Variable(pos1)\n",
    "          pos2 = Variable(pos2)\n",
    "          tags = Variable(tag)\n",
    "          \"\"\"\n",
    "          y = model(sentence,pos_tag,relation,pos1,pos2)  \n",
    "          #epoch_loss += loss.data.cpu().numpy()[0]\n",
    "          loss = criterion(y, tags)\n",
    "          total_loss += loss \n",
    "          #loss += l2_loss(params[1:]) * l2_rate\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()    \n",
    "          y = np.argmax(y.data.cpu().numpy(),axis=1)\n",
    "          #y = np.argmax(y.data.numpy(),axis=1)\n",
    "          #print(\"train_y:\",y)\n",
    "          #print(\"train_tag\",tag)\n",
    "          for y1,y2 in zip(y,tag):\n",
    "             # print(\"train_y1:\",y1)\n",
    "             # print(\"train_y2:\",y2)\n",
    "              if y1==y2:\n",
    "                  acc+=1\n",
    "\n",
    "              total+=1\n",
    "      train_p.append(100*float(acc)/total)\n",
    "      print (\"train:\",100*float(acc)/total,\"%\")\n",
    "      print(\"train_loss:\",(total_loss/total).data.cpu().numpy())\n",
    "     # print((total_loss/len(train_dataloader.dataset)).cpu().data.numpy())\n",
    "      #print((total_loss/len(train_dataloader.dataset)).data.numpy())\n",
    "      #print(loss.cpu().data.numpy()) \n",
    "\n",
    "    \n",
    "    \n",
    "      #start testing\n",
    "      acc_t=0\n",
    "      total_t=0\n",
    "      total_loss_t=0\n",
    "      count_predict = [0,0,0,0,0,0,0,0,0,0]\n",
    "      count_total = [0,0,0,0,0,0,0,0,0,0]\n",
    "      count_right = [0,0,0,0,0,0,0,0,0,0]\n",
    "      for sentence,pos_tag,relation,pos1,pos2,tag in test_dataloader:  \n",
    "\n",
    "          sentence = Variable(sentence.cuda()) \n",
    "          pos_tag = Variable(pos_tag.cuda()) \n",
    "          relation = Variable(relation.cuda())\n",
    "          pos1 = Variable(pos1.cuda())\n",
    "          pos2 = Variable(pos2.cuda())\n",
    "          tags = Variable(tag.cuda())\n",
    "          \"\"\"\n",
    "          sentence = Variable(sentence)\n",
    "          pos_tag = Variable(pos_tag)\n",
    "          relation = Variable(relation)\n",
    "          pos1 = Variable(pos1)\n",
    "          pos2 = Variable(pos2)\n",
    "          tag = Variable(tag)\n",
    "           \"\"\"\n",
    "          y = model(sentence,pos_tag,relation,relation,pos1,pos2)\n",
    "          loss = criterion(y, tags)\n",
    "          total_loss_t += loss \n",
    "\n",
    "          y = np.argmax(y.data.cpu().numpy(),axis=1)\n",
    "         # y = np.argmax(y.data.numpy(),axis=1)\n",
    "         # print('testy:',y)\n",
    "         # print('testtag:',tag)\n",
    "          for y1,y2 in zip(y,tag):\n",
    "             # print(\"test_y1:\",y1)\n",
    "              #print(\"test_y2:\",y2)\n",
    "              count_predict[y1]+=1\n",
    "              count_total[y2]+=1\n",
    "              if y1==y2:\n",
    "                  count_right[y1]+=1\n",
    "              total_t+=1\n",
    "\n",
    "      print(\"test_loss:\",(total_loss_t/total_t).data.cpu().numpy())\n",
    "      precision = [0,0,0,0,0,0,0,0,0,0]\n",
    "      recall = [0,0,0,0,0,0,0,0,0,0]\n",
    "      for i in range(len(count_predict)):\n",
    "          if count_predict[i]!=0 :\n",
    "              precision[i] = float(count_right[i])/count_predict[i]\n",
    "\n",
    "          if count_total[i]!=0:\n",
    "              recall[i] = float(count_right[i])/count_total[i]    \n",
    "\n",
    "      precision = sum(precision)/len(Relation2id)\n",
    "      recall = sum(recall)/len(Relation2id)\n",
    "      test_p.append(precision)\n",
    "      test_r.append(recall)\n",
    "      test_f.append((2*precision*recall)/(precision+recall))\n",
    "      print (\"准确率：\",precision)\n",
    "      print (\"召回率：\",recall)\n",
    "      print (\"f：\", (2*precision*recall)/(precision+recall))\n",
    "    #print(\"train_p:\",count(train_p)/len(train_p))\n",
    "    #print(\"test_p:\",count(test_p)/len(test_p))\n",
    "    #print(\"test_r:\",count(test_r)/len(test_r))\n",
    "    #print(\"test_f:\",count(test_f)/len(test_f))\n",
    "#start saveing\n",
    "torch.save(model, \"drive/论文/ChineseNRE-master更新后/data/model_BiLSTM_selfAtt_ch.pkl\")\n",
    "print (\"model has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5359,
     "status": "error",
     "timestamp": 1544260365231,
     "user": {
      "displayName": "李涛",
      "photoUrl": "",
      "userId": "13268993765785202327"
     },
     "user_tz": -480
    },
    "id": "SOROd_hqDTO9",
    "outputId": "df4a0918-b204-430c-ab58-13d039a90914"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-92050376f69d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "s =[1,2,3,4]\n",
    "print(count[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VmgxZxHZVgrT"
   },
   "outputs": [],
   "source": [
    "print(len(train_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "paEE5YLvU1e0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BiLSTM_SelfATT_ch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
